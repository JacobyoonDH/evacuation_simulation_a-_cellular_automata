{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b5fb96-5019-4007-94dd-a483959cd79b",
      "metadata": {
        "id": "b0b5fb96-5019-4007-94dd-a483959cd79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ffedf69-196a-4799-df90-4599a7550a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting python-dateutil\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: six, certifi, python-dateutil\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "Successfully installed certifi-2025.4.26 python-dateutil-2.9.0.post0 six-1.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "dateutil",
                  "six"
                ]
              },
              "id": "0cce9455f24c4272916ccfd8b91984ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiona==1.9.5\n",
            "  Downloading fiona-1.9.5-cp311-cp311-manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geopandas==0.13.2\n",
            "  Downloading geopandas-0.13.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting shapely==2.0.2\n",
            "  Downloading shapely-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting attrs>=19.2.0 (from fiona==1.9.5)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi (from fiona==1.9.5)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting click~=8.0 (from fiona==1.9.5)\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting click-plugins>=1.0 (from fiona==1.9.5)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cligj>=0.5 (from fiona==1.9.5)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting six (from fiona==1.9.5)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting setuptools (from fiona==1.9.5)\n",
            "  Downloading setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging (from geopandas==0.13.2)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pandas>=1.1.0 (from geopandas==0.13.2)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyproj>=3.0.1 (from geopandas==0.13.2)\n",
            "  Downloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas>=1.1.0->geopandas==0.13.2)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=1.1.0->geopandas==0.13.2)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.1.0->geopandas==0.13.2)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.9.5-cp311-cp311-manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geopandas-0.13.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shapely-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.4.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz, tzdata, six, setuptools, packaging, numpy, click, certifi, attrs, shapely, python-dateutil, pyproj, cligj, click-plugins, pandas, fiona, geopandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.1.0\n",
            "    Uninstalling shapely-2.1.0:\n",
            "      Successfully uninstalled shapely-2.1.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pyproj\n",
            "    Found existing installation: pyproj 3.7.1\n",
            "    Uninstalling pyproj-3.7.1:\n",
            "      Successfully uninstalled pyproj-3.7.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: geopandas\n",
            "    Found existing installation: geopandas 1.0.1\n",
            "    Uninstalling geopandas-1.0.1:\n",
            "      Successfully uninstalled geopandas-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "langchain-core 0.3.56 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-25.3.0 certifi-2025.4.26 click-8.2.0 click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.5 geopandas-0.13.2 numpy-1.26.4 packaging-25.0 pandas-2.2.3 pyproj-3.7.1 python-dateutil-2.9.0.post0 pytz-2025.2 setuptools-80.4.0 shapely-2.0.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi",
                  "dateutil",
                  "six"
                ]
              },
              "id": "ba54c12f7bab41de8082ee4624e02db4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 핵심 라이브러리만 정확히 맞춰 재설치\n",
        "!pip install --upgrade --force-reinstall certifi six python-dateutil\n",
        "\n",
        "!pip install --force-reinstall numpy==1.26.4 fiona==1.9.5 geopandas==0.13.2 shapely==2.0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import certifi, six, dateutil, numpy, fiona, geopandas, shapely\n",
        "\n",
        "print(\"certifi:\", certifi.__version__)\n",
        "print(\"six:\", six.__version__)\n",
        "print(\"python-dateutil:\", dateutil.__version__)\n",
        "print(\"numpy:\", numpy.__version__)\n",
        "print(\"fiona:\", fiona.__version__)\n",
        "print(\"geopandas:\", geopandas.__version__)\n",
        "print(\"shapely:\", shapely.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o40B37Sl1R7C",
        "outputId": "44c4ff1e-0bd9-42cf-e69d-f13feaa9c6a9"
      },
      "id": "o40B37Sl1R7C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "certifi: 2025.04.26\n",
            "six: 1.17.0\n",
            "python-dateutil: 2.9.0.post0\n",
            "numpy: 1.26.4\n",
            "fiona: 1.9.5\n",
            "geopandas: 0.13.2\n",
            "shapely: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sUk5VERbAhjo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUk5VERbAhjo",
        "outputId": "b44d0122-b1b2-4981-f566-15051b927e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/대피 시뮬레이션_최종코드.zip\n",
            "  inflating: /content/sim_code/aa.py  \n",
            "  inflating: /content/sim_code/grid_utils.py  \n",
            "  inflating: /content/sim_code/main.py  \n",
            "  inflating: /content/sim_code/pedestrian.py  \n",
            "  inflating: /content/sim_code/pedestrian_utils.py  \n",
            "  inflating: /content/sim_code/results_utils.py  \n",
            "  inflating: /content/sim_code/simulation.py  \n",
            "  inflating: /content/sim_code/simulation_itter.py  \n",
            "  inflating: /content/sim_code/visualization.py  \n",
            "  inflating: /content/sim_code/실행_AI학습데이터구축.ipynb  \n",
            "Archive:  /content/drive/MyDrive/데이터.zip\n",
            "  inflating: /content/sim_data/agent생성용.cpg  \n",
            "  inflating: /content/sim_data/agent생성용.dbf  \n",
            "  inflating: /content/sim_data/agent생성용.prj  \n",
            "  inflating: /content/sim_data/agent생성용.shp  \n",
            "  inflating: /content/sim_data/agent생성용.shx  \n",
            "  inflating: /content/sim_data/area_cells_cache.pkl  \n",
            "  inflating: /content/sim_data/flood_cells_grid.pkl  \n",
            "  inflating: /content/sim_data/grid.npy  \n",
            "  inflating: /content/sim_data/x_coords.csv  \n",
            "  inflating: /content/sim_data/y_coords.csv  \n",
            "  inflating: /content/sim_data/대상지_건물.cpg  \n",
            "  inflating: /content/sim_data/대상지_건물.dbf  \n",
            "  inflating: /content/sim_data/대상지_건물.prj  \n",
            "  inflating: /content/sim_data/대상지_건물.shp  \n",
            "  inflating: /content/sim_data/대상지_건물.shx  \n",
            " extracting: /content/sim_data/대피소200m버퍼.cpg  \n",
            "  inflating: /content/sim_data/대피소200m버퍼.dbf  \n",
            "  inflating: /content/sim_data/대피소200m버퍼.prj  \n",
            "  inflating: /content/sim_data/대피소200m버퍼.shp  \n",
            "  inflating: /content/sim_data/대피소200m버퍼.shx  \n",
            " extracting: /content/sim_data/대피소_shp.cpg  \n",
            "  inflating: /content/sim_data/대피소_shp.dbf  \n",
            "  inflating: /content/sim_data/대피소_shp.prj  \n",
            "  inflating: /content/sim_data/대피소_shp.shp  \n",
            "  inflating: /content/sim_data/대피소_shp.shx  \n",
            "  inflating: /content/sim_data/시장영역.cpg  \n",
            "  inflating: /content/sim_data/시장영역.dbf  \n",
            "  inflating: /content/sim_data/시장영역.prj  \n",
            "  inflating: /content/sim_data/시장영역.shp  \n",
            "  inflating: /content/sim_data/시장영역.shx  \n",
            " extracting: /content/sim_data/신규대상지_도보이동가능지역.cpg  \n",
            "  inflating: /content/sim_data/신규대상지_도보이동가능지역.dbf  \n",
            "  inflating: /content/sim_data/신규대상지_도보이동가능지역.prj  \n",
            "  inflating: /content/sim_data/신규대상지_도보이동가능지역.shp  \n",
            "  inflating: /content/sim_data/신규대상지_도보이동가능지역.shx  \n",
            " extracting: /content/sim_data/침수지역.cpg  \n",
            "  inflating: /content/sim_data/침수지역.dbf  \n",
            "  inflating: /content/sim_data/침수지역.prj  \n",
            "  inflating: /content/sim_data/침수지역.shp  \n",
            "  inflating: /content/sim_data/침수지역.shx  \n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 압축 해제\n",
        "!unzip \"/content/drive/MyDrive/대피 시뮬레이션_최종코드.zip\" -d /content/sim_code\n",
        "!unzip \"/content/drive/MyDrive/데이터.zip\" -d /content/sim_data\n",
        "\n",
        "# 코드 불러올 수 있게 path 추가\n",
        "import sys\n",
        "sys.path.append('/content/sim_code')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GivmqS1LA0ho",
      "metadata": {
        "id": "GivmqS1LA0ho"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/sim_code')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c21cf07-d57d-40a8-99f4-2cb8400d1911",
      "metadata": {
        "id": "5c21cf07-d57d-40a8-99f4-2cb8400d1911"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import matplotlib.patches as mpatches\n",
        "from grid_utils import *\n",
        "from pedestrian_utils import *\n",
        "from pedestrian import *\n",
        "from simulation import *\n",
        "#from results_utils import *\n",
        "#from visualization import *\n",
        "import numpy as np\n",
        "from simulation import *\n",
        "#from results_utils import *\n",
        "#from visualization import *\n",
        "from pedestrian_utils import *\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FvHA1CroBen4",
      "metadata": {
        "id": "FvHA1CroBen4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd62064-8c96-45e9-ab8a-0107eeec7290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Reprojection complete. All data is now in EPSG:5181.\n"
          ]
        }
      ],
      "source": [
        "plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows용 폰트, Colab에서는 기본 폰트 사용 권장\n",
        "\n",
        "# 경로 변경 (Colab 기준)\n",
        "base_data_path = \"/content/sim_data\"\n",
        "\n",
        "area_shp_path = f\"{base_data_path}/신규대상지_도보이동가능지역.shp\"\n",
        "buildings_shp_path = f\"{base_data_path}/대상지_건물.shp\"\n",
        "special_area_shp_path = f\"{base_data_path}/시장영역.shp\"\n",
        "evacuation_target_area_shp_path = f\"{base_data_path}/대피소_shp.shp\"\n",
        "creation_area_path = f\"{base_data_path}/agent생성용.shp\"\n",
        "grid_path = base_data_path\n",
        "flood_path = f\"{base_data_path}/flood_cells_grid.pkl\"\n",
        "\n",
        "# 좌표계 통일\n",
        "target_crs = \"EPSG:5181\"\n",
        "area_gdf = gpd.read_file(area_shp_path).to_crs(target_crs)\n",
        "buildings_gdf = gpd.read_file(buildings_shp_path).to_crs(target_crs)\n",
        "special_area_gdf = gpd.read_file(special_area_shp_path).to_crs(target_crs)\n",
        "evacuation_target_area_gdf = gpd.read_file(evacuation_target_area_shp_path).to_crs(target_crs)\n",
        "creation_area_gdf = gpd.read_file(creation_area_path).to_crs(target_crs)\n",
        "\n",
        "print(\"✅ Reprojection complete. All data is now in EPSG:5181.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b669a7-6800-49f7-8f27-c8d5cb257aea",
      "metadata": {
        "id": "97b669a7-6800-49f7-8f27-c8d5cb257aea"
      },
      "outputs": [],
      "source": [
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850900b3-1596-4593-8dd7-e39d6eca260c",
      "metadata": {
        "id": "850900b3-1596-4593-8dd7-e39d6eca260c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd7f908-09cb-4f76-d964-90985477448f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully loaded.\n",
            "Flood cells grid loaded from /content/sim_data/flood_cells_grid.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "def load_area_cells():\n",
        "    if os.path.exists(cache_file):\n",
        "        with open(cache_file, \"rb\") as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "            creation_area_cells = cached_data[\"creation_area_cells\"]\n",
        "            special_area_cells = cached_data[\"special_area_cells\"]\n",
        "            evacuation_target_area_cells = cached_data[\"evacuation_target_area_cells\"]\n",
        "            general_area_cells = creation_area_cells  # general_area_cells는 creation_area_cells와 동일\n",
        "\n",
        "            print(\"Data successfully loaded.\")\n",
        "\n",
        "            return creation_area_cells, special_area_cells, evacuation_target_area_cells, general_area_cells\n",
        "    else:\n",
        "        print(\"Cache file not found. Please generate and save the data first.\")\n",
        "        return None, None, None, None\n",
        "cache_file = \"/content/sim_data/area_cells_cache.pkl\"\n",
        "flood_cells_file_path = \"/content/sim_data/flood_cells_grid.pkl\"\n",
        "\n",
        "creation_area_cells, special_area_cells, evacuation_target_area_cells, general_area_cells = load_area_cells()\n",
        "def load_flood_cells_grid(file_path):\n",
        "    \"\"\"\n",
        "    저장된 Flood cells grid를 로드합니다.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): 로드할 파일 경로\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: 로드된 flood cells grid\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"{file_path} does not exist.\")\n",
        "    with open(file_path, 'rb') as f:\n",
        "        flood_cells_grid = pickle.load(f)\n",
        "    print(f\"Flood cells grid loaded from {file_path}\")\n",
        "    return flood_cells_grid\n",
        "# 예제 실행\n",
        "# flood_gdf 정의 필요\n",
        "# Flood cells grid 로드\n",
        "loaded_flood_cells_grid = load_flood_cells_grid(flood_cells_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9ab957-9387-4f69-b571-6ea771d0520b",
      "metadata": {
        "id": "2a9ab957-9387-4f69-b571-6ea771d0520b"
      },
      "outputs": [],
      "source": [
        "def initialize_pedestrian_positions(total_agents, special_area_cells, general_area_cells, special_area_ratio):\n",
        "    \"\"\"\n",
        "    Initialize pedestrian positions in special and general areas, excluding evacuation targets.\n",
        "\n",
        "    Parameters:\n",
        "        total_agents (int): Total number of agents to generate.\n",
        "        special_area_cells (list): List of special area cells (excludes evacuation targets).\n",
        "        general_area_cells (list): List of general area cells (excludes evacuation targets).\n",
        "        special_area_ratio (float): Ratio of agents in special areas.\n",
        "\n",
        "    Returns:\n",
        "        list: List of initial positions for each pedestrian.\n",
        "    \"\"\"\n",
        "    # Calculate the number of agents in special and general areas\n",
        "    num_special_area_agents = int(total_agents * special_area_ratio)\n",
        "    num_general_area_agents = total_agents - num_special_area_agents\n",
        "\n",
        "    # Randomly select cells for special and general area agents\n",
        "    selected_special_area_cells = random.sample(special_area_cells, num_special_area_agents)\n",
        "    selected_general_area_cells = random.sample(general_area_cells, num_general_area_agents)\n",
        "\n",
        "    # Combine and shuffle all selected cells\n",
        "    all_selected_cells = selected_special_area_cells + selected_general_area_cells\n",
        "    random.shuffle(all_selected_cells)\n",
        "\n",
        "    return all_selected_cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee93826b-9151-4008-a36f-ecc5c38d9df6",
      "metadata": {
        "id": "ee93826b-9151-4008-a36f-ecc5c38d9df6"
      },
      "outputs": [],
      "source": [
        "def create_pedestrians_with_behaviors(total_agents, agent_type_ratios, agent_types, behaviors, positions):\n",
        "    \"\"\"\n",
        "    Create pedestrians with dynamic properties such as behavior based on predefined positions.\n",
        "\n",
        "    Parameters:\n",
        "        total_agents (int): Total number of agents.\n",
        "        agent_type_ratios (dict): Ratio of different agent types.\n",
        "        agent_types (dict): Speeds associated with agent types.\n",
        "        behaviors (dict): Behavioral patterns for the simulation.\n",
        "        positions (list): Predefined positions for each agent.\n",
        "\n",
        "    Returns:\n",
        "        list: List of pedestrian objects with assigned behaviors and properties.\n",
        "    \"\"\"\n",
        "    # Calculate distribution of agent types\n",
        "    type_distribution = []\n",
        "    for agent_type, ratio in agent_type_ratios.items():\n",
        "        num_type_agents = int(total_agents * ratio)\n",
        "        type_distribution.extend([agent_type] * num_type_agents)\n",
        "    random.shuffle(type_distribution)\n",
        "\n",
        "    # Create pedestrian objects\n",
        "    pedestrians = []\n",
        "    for i, position in enumerate(positions):\n",
        "        if i >= total_agents:\n",
        "            break\n",
        "        agent_type = type_distribution[i]\n",
        "        speed = agent_types[agent_type]\n",
        "        behavior = random.choices(list(behaviors.keys()), weights=behaviors.values(), k=1)[0]\n",
        "        special_area = position in special_area_cells  # Assume special_area_cells is accessible\n",
        "        pedestrian = Pedestrian(position, speed, behavior, agent_type, special_area, id=i)\n",
        "        pedestrians.append(pedestrian)\n",
        "\n",
        "    return pedestrians\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0502c567-efe1-4ff9-bc77-bb67ab1eaec9",
      "metadata": {
        "id": "0502c567-efe1-4ff9-bc77-bb67ab1eaec9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def run_simulation_scenario_for_debug_and_save(special_area_ratio, initial_positions):\n",
        "    total_agents = 1000\n",
        "    agent_type_ratios = {'노인/어린이': 0.29, '중장년': 0.34, '청소년/청년': 0.37, '장애인': 0.07}\n",
        "    agent_types = {'노인/어린이': 1.0, '중장년': 1.3, '청소년/청년': 1.9, '장애인': 0.71}\n",
        "\n",
        "    shelter_knowledge_probabilities = [0.3, 0.6, 0.9]\n",
        "    exploratory_ratios = [0.1, 0.4, 0.7]\n",
        "    knows_ratios = [round((1 - exploratory) / 2, 2) for exploratory in exploratory_ratios]\n",
        "\n",
        "    types_distribution = []\n",
        "    for agent_type, ratio in agent_type_ratios.items():\n",
        "        types_distribution.extend([agent_type] * int(ratio * total_agents))\n",
        "    random.shuffle(types_distribution)\n",
        "\n",
        "    # ✅ 저장 경로 및 접두어 설정 (Colab용 Google Drive 경로)\n",
        "    save_dir = \"/content/drive/MyDrive/시뮬레이션결과\"\n",
        "    prefix = \"초기혼잡X_\" if special_area_ratio == 0 else \"초기혼잡O_\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for knowledge_prob in shelter_knowledge_probabilities:\n",
        "        for exploratory, knows in zip(exploratory_ratios, knows_ratios):\n",
        "            behaviors = {'knows_specific': knows, 'knows_all': knows, 'exploratory': exploratory}\n",
        "\n",
        "            for trial in range(1, 11):\n",
        "              file_name = f\"{prefix}대피소정보전파확률_{knowledge_prob}_대피소인지비율{round(knows*2, 2)}_trial{trial}.xlsx\"\n",
        "              file_path = os.path.join(save_dir, file_name)\n",
        "              # ✅ 파일 존재 시 스킵\n",
        "              if os.path.exists(file_path):\n",
        "                print(f\"✅ 이미 존재: {file_name} → 건너뜀\")\n",
        "                continue\n",
        "              #print(f\"\\n🚀 [실행] {file_name}\")\n",
        "\n",
        "              pedestrians = [\n",
        "                    Pedestrian(\n",
        "                        pos, agent_types[agent_type],\n",
        "                        random.choices(list(behaviors.keys()), weights=list(behaviors.values()), k=1)[0],\n",
        "                        agent_type, special_area=False, id=j,\n",
        "                        shelter_knowledge_probability=knowledge_prob\n",
        "                    )\n",
        "                    for j, (pos, agent_type) in enumerate(zip(initial_positions, types_distribution))\n",
        "                ]\n",
        "\n",
        "              run_simulation(\n",
        "                    grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords,\n",
        "                    area_gdf, buildings_gdf, steps=1000, evacuee_area=None,\n",
        "                    save_path=None, flood_cells=loaded_flood_cells_grid\n",
        "                )\n",
        "\n",
        "                # ✅ 저장 파일 이름 구성 (논문 제출 형식 반영)\n",
        "#                file_name = f\"{prefix}대피소정보전파확률_{knowledge_prob}_대피소인지비율{round(knows*2, 2)}_trial{trial}.xlsx\"\n",
        " #               file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "                # ✅ DataFrame 저장\n",
        "              data = []\n",
        "              for ped in pedestrians:\n",
        "                  d = vars(ped).copy()\n",
        "                  for k, v in d.items():\n",
        "                      if isinstance(v, (list, tuple, dict)):\n",
        "                          d[k] = str(v)\n",
        "                  data.append(d)\n",
        "\n",
        "              df = pd.DataFrame(data)\n",
        "              df.to_excel(file_path, index=False)\n",
        "              print(f\"✅ 저장 완료: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0af695-0d11-479a-9392-e14a6e906f40",
      "metadata": {
        "id": "dc0af695-0d11-479a-9392-e14a6e906f40"
      },
      "outputs": [],
      "source": [
        "initial_positions = initialize_pedestrian_positions(\n",
        "    total_agents=1000,\n",
        "    special_area_cells=special_area_cells,\n",
        "    general_area_cells=general_area_cells,\n",
        "    special_area_ratio=0\n",
        ")\n",
        "\n",
        "#run_simulation_scenario_for_debug_and_save(0, initial_positions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def run_single_simulation_scenario(special_area_ratio, initial_positions, knowledge_prob, awareness_ratio, trial):\n",
        "    total_agents = 1000\n",
        "    agent_type_ratios = {'노인/어린이': 0.29, '중장년': 0.34, '청소년/청년': 0.37, '장애인': 0.07}\n",
        "    agent_types = {'노인/어린이': 1.0, '중장년': 1.3, '청소년/청년': 1.9, '장애인': 0.71}\n",
        "\n",
        "    exploratory = round(1 - awareness_ratio, 2)\n",
        "    knows = round(awareness_ratio / 2, 2)\n",
        "    behaviors = {'knows_specific': knows, 'knows_all': knows, 'exploratory': exploratory}\n",
        "\n",
        "    types_distribution = []\n",
        "    for agent_type, ratio in agent_type_ratios.items():\n",
        "        types_distribution.extend([agent_type] * int(ratio * total_agents))\n",
        "    random.shuffle(types_distribution)\n",
        "\n",
        "    # ✅ 저장 경로 및 파일명 설정\n",
        "    save_dir = \"/content/drive/MyDrive/시뮬레이션결과\"\n",
        "    prefix = \"초기혼잡X_\" if special_area_ratio == 0 else \"초기혼잡O_\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    file_name = f\"{prefix}대피소정보전파확률_{knowledge_prob}_대피소인지비율{awareness_ratio}_trial{trial}.xlsx\"\n",
        "    file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"✅ 이미 존재: {file_name} → 건너뜀\")\n",
        "        return\n",
        "\n",
        "    # ✅ 시뮬레이션 실행\n",
        "    pedestrians = [\n",
        "        Pedestrian(\n",
        "            pos, agent_types[agent_type],\n",
        "            random.choices(list(behaviors.keys()), weights=list(behaviors.values()), k=1)[0],\n",
        "            agent_type, special_area=False, id=j,\n",
        "            shelter_knowledge_probability=knowledge_prob\n",
        "        )\n",
        "        for j, (pos, agent_type) in enumerate(zip(initial_positions, types_distribution))\n",
        "    ]\n",
        "\n",
        "    run_simulation(\n",
        "        grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords,\n",
        "        area_gdf, buildings_gdf, steps=1000, evacuee_area=None,\n",
        "        save_path=None, flood_cells=loaded_flood_cells_grid\n",
        "    )\n",
        "\n",
        "    # ✅ 결과 저장\n",
        "    data = []\n",
        "    for ped in pedestrians:\n",
        "        d = vars(ped).copy()\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, (list, tuple, dict)):\n",
        "                d[k] = str(v)\n",
        "        data.append(d)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(file_path, index=False)\n",
        "    print(f\"✅ 저장 완료: {file_path}\")\n"
      ],
      "metadata": {
        "id": "vRzjnyAjsJpE"
      },
      "id": "vRzjnyAjsJpE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 재수행 대상 리스트 (필요 시 이 아래에 전체 41개 항목 입력)\n",
        "# 누락된 시나리오 목록 기반으로, 재수행 대상 리스트를 생성 (5건 제외)\n",
        "# 아래 리스트는 사용자가 이전에 제공한 df_missing_cases에서 파생되었고, 상위 5건은 이미 수행되었음\n",
        "rerun_targets = [\n",
        "    (0.9, 0.6, i) for i in range(1, 10)\n",
        "] + [\n",
        "    (0.9, 0.9, i) for i in range(1, 10)\n",
        "]\n",
        "\n",
        "\n",
        "# ✅ 시뮬레이션 반복 실행\n",
        "for info_prob, awareness_ratio, trial in rerun_targets:\n",
        "    run_single_simulation_scenario(\n",
        "        special_area_ratio=0,  # 초기혼잡X\n",
        "        initial_positions=initial_positions,\n",
        "        knowledge_prob=info_prob,\n",
        "        awareness_ratio=awareness_ratio,\n",
        "        trial=trial\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8HFLet55sJye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6e5f36-0cb2-42b3-ea54-0e6ce9285b89"
      },
      "id": "8HFLet55sJye",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial1.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial2.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial3.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial4.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial5.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial6.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial7.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial8.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.6_trial9.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial1.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial2.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial3.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial4.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial5.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial6.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial7.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial8.xlsx\n",
            "✅ 저장 완료: /content/drive/MyDrive/시뮬레이션결과/초기혼잡X_대피소정보전파확률_0.9_대피소인지비율0.9_trial9.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3dSOg33sJ1r"
      },
      "id": "m3dSOg33sJ1r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmULrofwsJ4j"
      },
      "id": "UmULrofwsJ4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f122ab25-e520-471b-a7ed-e5eff2791c66",
      "metadata": {
        "id": "f122ab25-e520-471b-a7ed-e5eff2791c66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087c12c3-42ea-455a-aa11-21cabab44294",
      "metadata": {
        "id": "087c12c3-42ea-455a-aa11-21cabab44294"
      },
      "outputs": [],
      "source": [
        "asasasasas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90242c0d-f6bf-4eca-ba55-24a536f9af48",
      "metadata": {
        "id": "90242c0d-f6bf-4eca-ba55-24a536f9af48"
      },
      "source": [
        "### 최종 논문용 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ba8097-98eb-441c-b614-756327cae691",
      "metadata": {
        "id": "e9ba8097-98eb-441c-b614-756327cae691"
      },
      "outputs": [],
      "source": [
        "끝내"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c896c9-4936-4316-afb0-bc9fd2584ad4",
      "metadata": {
        "id": "55c896c9-4936-4316-afb0-bc9fd2584ad4"
      },
      "outputs": [],
      "source": [
        "flood_cells = loaded_flood_cells_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402dc648-7764-4d60-8ba5-c52ec51e26c6",
      "metadata": {
        "id": "402dc648-7764-4d60-8ba5-c52ec51e26c6"
      },
      "outputs": [],
      "source": [
        "# 초기 설정: 보행자 생성 파라미터\n",
        "total_agents = 1000\n",
        "special_area_ratio = 0.3\n",
        "agent_type_ratios = {'노인/어린이': 0.29, '중장년': 0.34, '청소년/청년': 0.37, '장애인': 0.07}\n",
        "agent_types = {'노인/어린이': 1.0, '중장년': 1.3, '청소년/청년': 1.9, '장애인': 0.71}\n",
        "\n",
        "# 시뮬레이션 설정: behaviors 비율\n",
        "simulation_count = 9\n",
        "exploratory_ratios = [0.1 * i for i in range(1, 10)]  # 0.1부터 0.9까지\n",
        "knows_ratios = [round((1 - exploratory) / 2, 2) for exploratory in exploratory_ratios]  # 나머지 비율을 균등하게 분배\n",
        "\n",
        "# 결과 저장 경로 설정\n",
        "base_path = r'C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과'\n",
        "\n",
        "# 시뮬레이션 반복\n",
        "for i, (exploratory, knows) in enumerate(zip(exploratory_ratios, knows_ratios), start=1):\n",
        "    # behaviors 설정\n",
        "    behaviors = {'knows_specific': knows, 'knows_all': knows, 'exploratory': exploratory}\n",
        "    print(f\"\\nRunning simulation {i} with behaviors: {behaviors}\")\n",
        "\n",
        "    # 보행자 생성\n",
        "    pedestrians = generate_pedestrians(\n",
        "        total_agents, agent_type_ratios, agent_types, behaviors,\n",
        "        special_area_cells, general_area_cells, evacuation_target_area_cells,\n",
        "        x_coords, y_coords, special_area_ratio\n",
        "    )\n",
        "\n",
        "    # 시뮬레이션 실행\n",
        "    dynamic_congestion_grid, accumulated_congestion_grid, evacuation_details, max_congestion_per_step = run_simulation(\n",
        "        grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords,\n",
        "        area_gdf, buildings_gdf, steps=1000, evacuee_area=None, save_path=None, flood_cells=flood_cells\n",
        "    )\n",
        "\n",
        "    # 결과 저장 경로 설정\n",
        "    excel_filename = os.path.join(base_path, f'simulation_results_{i}.xlsx')\n",
        "    dynamic_image_path = os.path.join(base_path, f'dynamic_congestion_{i}.png')\n",
        "    accumulated_image_path = os.path.join(base_path, f'accumulated_congestion_{i}.png')\n",
        "\n",
        "    # Excel 결과 저장\n",
        "    save_results_to_excel(\n",
        "        pedestrians=pedestrians,\n",
        "        filename=excel_filename,\n",
        "        accumulated_congestion_grid=accumulated_congestion_grid,\n",
        "        max_congestion_per_step=max_congestion_per_step\n",
        "    )\n",
        "\n",
        "    # 그림 저장\n",
        "    visualize_accumulated_congestion(\n",
        "    area_gdf, buildings_gdf, evacuation_target_area_gdf,\n",
        "    accumulated_congestion_grid, x_coords, y_coords,\n",
        "    save_path=accumulated_image_path\n",
        "    )\n",
        "\n",
        "    visualize_accumulated_congestion(\n",
        "        area_gdf, buildings_gdf, evacuation_target_area_gdf,\n",
        "        dynamic_congestion_grid, x_coords, y_coords,\n",
        "        save_path=dynamic_image_path\n",
        "    )\n",
        "    print(f\"Simulation {i} completed and results saved to:\\n- {excel_filename}\\n- {dynamic_image_path}\\n- {accumulated_image_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1943c60d-ed85-4db7-9dd3-bfc08be1ea2f",
      "metadata": {
        "id": "1943c60d-ed85-4db7-9dd3-bfc08be1ea2f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4fd73e82-3ce9-46ca-8cb3-3257b0e8dcab",
      "metadata": {
        "id": "4fd73e82-3ce9-46ca-8cb3-3257b0e8dcab"
      },
      "source": [
        "# 혼잡도 재확인 1m^2당 기준 (격자 1나씩만)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0282ae1-b5e8-4904-96cc-fd9bbda19b09",
      "metadata": {
        "id": "c0282ae1-b5e8-4904-96cc-fd9bbda19b09"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\데이터\"\n",
        "\n",
        "# 격자 데이터 로드\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# 시뮬레이션 데이터 파일\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# 시간대별 격자 혼잡도 계산 함수\n",
        "def calculate_congestion_per_time_step(congestion_paths):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        positions_at_t = []\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                position = path[t]\n",
        "                if isinstance(position, tuple):  # 데이터가 tuple인지 확인\n",
        "                    positions_at_t.append(position)\n",
        "\n",
        "        position_counts = Counter(positions_at_t)\n",
        "        time_step_congestion.append(position_counts)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# 혼잡 경로 데이터 로드 및 혼잡도 계산\n",
        "data = pd.read_excel(simulation_file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "time_step_congestion = calculate_congestion_per_time_step(congestion_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82316f16-fbe8-4ffa-bb4a-0b7f58232a73",
      "metadata": {
        "id": "82316f16-fbe8-4ffa-bb4a-0b7f58232a73"
      },
      "outputs": [],
      "source": [
        "max_congestion = max(time_step_congestion[0].items(), key=lambda x: x[1])\n",
        "print(f\"최대 혼잡도: {max_congestion[1]}, 위치: {max_congestion[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e8a5d5-ca98-437b-a7b6-005dcc428a5b",
      "metadata": {
        "id": "35e8a5d5-ca98-437b-a7b6-005dcc428a5b"
      },
      "source": [
        "# 횬쟙도 재확인 3x3기준으로 m^2/인 도츨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2cc140-7193-43bc-9e96-4823f21e43c4",
      "metadata": {
        "id": "fc2cc140-7193-43bc-9e96-4823f21e43c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\데이터\"\n",
        "\n",
        "# 격자 데이터 로드\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore 이웃 기반 혼잡도 계산 함수\n",
        "def calculate_congestion_with_moore(congestion_paths, grid_shape):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        congestion_map = np.zeros(grid_shape)\n",
        "\n",
        "        # 각 agent의 위치를 업데이트\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                x, y = path[t]\n",
        "                if 0 <= x < grid_shape[1] and 0 <= y < grid_shape[0]:\n",
        "                    congestion_map[y, x] += 1\n",
        "\n",
        "        # Moore 이웃 계산\n",
        "        extended_congestion_map = np.zeros_like(congestion_map)\n",
        "        for y in range(grid_shape[0]):\n",
        "            for x in range(grid_shape[1]):\n",
        "                # 3x3 영역의 합 계산\n",
        "                for dy in range(-1, 2):\n",
        "                    for dx in range(-1, 2):\n",
        "                        ny, nx = y + dy, x + dx\n",
        "                        if 0 <= nx < grid_shape[1] and 0 <= ny < grid_shape[0]:\n",
        "                            extended_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "\n",
        "        # 면적 대비 혼잡도 계산 (m^2/인)\n",
        "        cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # 한 격자의 면적\n",
        "        extended_congestion_map /= cell_area\n",
        "        time_step_congestion.append(extended_congestion_map)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# 혼잡 경로 데이터 로드 및 혼잡도 계산\n",
        "data = pd.read_excel(os.path.join(base_path, \"simulation_results_1.xlsx\"), sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "time_step_congestion = calculate_congestion_with_moore(congestion_paths, grid.shape)\n",
        "\n",
        "# 특정 time step 혼잡도 확인 (예: 첫 번째 time step)\n",
        "time_step = 0\n",
        "print(f\"혼잡도 맵 (time step {time_step + 1}):\")\n",
        "print(time_step_congestion[time_step])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2687f6f5-e7ae-44fa-9fbb-14bbfe56030f",
      "metadata": {
        "id": "2687f6f5-e7ae-44fa-9fbb-14bbfe56030f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\데이터\"\n",
        "\n",
        "# 격자 데이터 로드\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore 이웃 기반 혼잡도 계산 함수\n",
        "def calculate_congestion_with_moore(congestion_paths, grid_shape):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        congestion_map = np.zeros(grid_shape)\n",
        "\n",
        "        # 각 agent의 위치를 업데이트\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                x, y = path[t]\n",
        "                if 0 <= x < grid_shape[1] and 0 <= y < grid_shape[0]:\n",
        "                    congestion_map[y, x] += 1\n",
        "\n",
        "        # Moore 이웃 계산\n",
        "        extended_congestion_map = np.zeros_like(congestion_map)\n",
        "        for y in range(grid_shape[0]):\n",
        "            for x in range(grid_shape[1]):\n",
        "                # 3x3 영역의 합 계산 (가능한 영역만 포함)\n",
        "                for dy in range(-1, 2):\n",
        "                    for dx in range(-1, 2):\n",
        "                        ny, nx = y + dy, x + dx\n",
        "                        if 0 <= nx < grid_shape[1] and 0 <= ny < grid_shape[0]:\n",
        "                            extended_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "\n",
        "        # 면적 대비 혼잡도 계산 (m^2/인)\n",
        "        cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # 한 격자의 면적\n",
        "        extended_congestion_map /= cell_area\n",
        "        time_step_congestion.append(extended_congestion_map)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# 시뮬레이션 파일 처리\n",
        "simulation_files = [os.path.join(base_path, f\"simulation_results_{i}.xlsx\") for i in range(1, 10)]\n",
        "all_simulations = []\n",
        "\n",
        "for i, file in enumerate(simulation_files, start=1):\n",
        "    try:\n",
        "        # 데이터 로드\n",
        "        data = pd.read_excel(file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "        congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "        time_step_congestion = calculate_congestion_with_moore(congestion_paths, grid.shape)\n",
        "\n",
        "        # 각 time step의 평균 혼잡도 계산\n",
        "        avg_congestions = [\n",
        "            np.mean(step) if step.size > 0 else 0  # 평균 혼잡도 계산\n",
        "            for step in time_step_congestion\n",
        "        ]\n",
        "\n",
        "        # 길이 고정: 1000개의 time step으로 맞추기\n",
        "        if len(avg_congestions) < 1000:\n",
        "            avg_congestions.extend([avg_congestions[-1]] * (1000 - len(avg_congestions)))\n",
        "        else:\n",
        "            avg_congestions = avg_congestions[:1000]\n",
        "\n",
        "        time_steps = list(range(1, 1001))\n",
        "\n",
        "        # DataFrame 생성\n",
        "        df = pd.DataFrame({\n",
        "            \"Time Step\": time_steps,\n",
        "            f\"대피소 인지 비율 {100 - i * 10}%\": avg_congestions\n",
        "        })\n",
        "\n",
        "        if len(all_simulations) == 0:\n",
        "            all_simulations = df\n",
        "        else:\n",
        "            all_simulations = pd.merge(all_simulations, df, on=\"Time Step\", how=\"outer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# 하나의 시트로 저장\n",
        "output_file = os.path.join(base_path, \"혼잡도계산_확장기준.xlsx\")\n",
        "all_simulations.to_excel(output_file, index=False)\n",
        "print(f\"모든 시뮬레이션 결과가 {output_file}에 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71bc7f84-f6ab-492c-a9ed-373c082c0462",
      "metadata": {
        "id": "71bc7f84-f6ab-492c-a9ed-373c082c0462"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\데이터\"\n",
        "\n",
        "# 격자 데이터 로드\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore 이웃 혼잡도 계산 함수 (혼잡도가 존재하는 셀만 계산)\n",
        "def calculate_moore_congestion(grid, agent_positions):\n",
        "    congestion_map = np.zeros_like(grid, dtype=float)  # float 타입으로 초기화\n",
        "\n",
        "    # agent 위치를 grid에 맵핑\n",
        "    for x, y in agent_positions:\n",
        "        if 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:\n",
        "            congestion_map[y, x] += 1\n",
        "\n",
        "    # Moore 이웃 계산 (혼잡도가 존재하는 셀만)\n",
        "    moore_congestion_map = np.zeros_like(congestion_map, dtype=float)  # float 타입으로 초기화\n",
        "    active_cells = np.argwhere(congestion_map > 0)\n",
        "\n",
        "    for y, x in active_cells:\n",
        "        cell_count = 0  # 포함된 셀 개수\n",
        "        for dy in range(-1, 2):\n",
        "            for dx in range(-1, 2):\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < grid.shape[0] and 0 <= nx < grid.shape[1]:\n",
        "                    moore_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "                    cell_count += 1  # 포함된 셀 수 증가\n",
        "        if cell_count > 0:\n",
        "            moore_congestion_map[y, x] /= cell_count  # 셀 개수로 나눔\n",
        "\n",
        "    return moore_congestion_map\n",
        "\n",
        "# 시나리오 1 데이터 처리\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "try:\n",
        "    # 데이터 로드\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "\n",
        "    # 시간대별 평균 혼잡도 저장\n",
        "    avg_congestions = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        # agent 위치 복원\n",
        "        agent_positions = [path[t] for path in congestion_paths if t < len(path)]\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions)\n",
        "        avg_congestions.append(np.mean(moore_congestion_map[moore_congestion_map > 0]))\n",
        "\n",
        "    # 길이 고정: 1000개의 time step으로 맞추기\n",
        "    if len(avg_congestions) < 1000:\n",
        "        avg_congestions.extend([avg_congestions[-1]] * (1000 - len(avg_congestions)))\n",
        "    else:\n",
        "        avg_congestions = avg_congestions[:1000]\n",
        "\n",
        "    time_steps = list(range(1, 1001))\n",
        "\n",
        "    # DataFrame 생성\n",
        "    df = pd.DataFrame({\n",
        "        \"Time Step\": time_steps,\n",
        "        \"대피소 인지 비율 90%\": avg_congestions\n",
        "    })\n",
        "\n",
        "    # 결과 저장\n",
        "    output_file = os.path.join(base_path, \"혼잡도계산_최적화_시나리오1.xlsx\")\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"시나리오 1 결과가 {output_file}에 저장되었습니다.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13dd00a6-2620-4f16-995e-6f6fdc645788",
      "metadata": {
        "id": "13dd00a6-2620-4f16-995e-6f6fdc645788"
      },
      "outputs": [],
      "source": [
        "moore_congestion_map\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31896c98-9480-42ac-986f-57629199ed9e",
      "metadata": {
        "id": "31896c98-9480-42ac-986f-57629199ed9e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\데이터\"\n",
        "\n",
        "# 격자 데이터 로드\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore 이웃 혼잡도 계산 함수 (혼잡도가 존재하는 셀만 계산)\n",
        "def calculate_moore_congestion(grid, agent_positions, cell_area):\n",
        "    congestion_map = np.zeros_like(grid, dtype=float)  # float 타입으로 초기화\n",
        "\n",
        "    # agent 위치를 grid에 맵핑\n",
        "    for x, y in agent_positions:\n",
        "        if 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:\n",
        "            congestion_map[y, x] += 1\n",
        "\n",
        "    # Moore 이웃 계산 (혼잡도가 존재하는 셀만)\n",
        "    moore_congestion_map = np.zeros_like(congestion_map, dtype=float)  # float 타입으로 초기화\n",
        "    active_cells = np.argwhere(congestion_map > 0)\n",
        "\n",
        "    for y, x in active_cells:\n",
        "        cell_count = 0  # 포함된 셀 개수\n",
        "        for dy in range(-1, 2):\n",
        "            for dx in range(-1, 2):\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < grid.shape[0] and 0 <= nx < grid.shape[1]:\n",
        "                    moore_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "                    cell_count += 1  # 포함된 셀 수 증가\n",
        "        if cell_count > 0:\n",
        "            moore_congestion_map[y, x] /= (cell_area * cell_count)  # \\( \\text{m}^2/\\text{인} \\) 계산\n",
        "\n",
        "    return moore_congestion_map\n",
        "\n",
        "# 시나리오 1 데이터 처리 및 시각화\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "try:\n",
        "    # 데이터 로드\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "\n",
        "    # 셀 면적 계산\n",
        "    cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # \\( \\text{m}^2 \\)\n",
        "\n",
        "    # 시각화할 time step 목록\n",
        "    time_steps_to_visualize = [0, 499, 999]  # 0초, 500초, 1000초\n",
        "    for time_step in time_steps_to_visualize:\n",
        "        # agent 위치 복원\n",
        "        agent_positions = [path[time_step] for path in congestion_paths if time_step < len(path)]\n",
        "\n",
        "        # Moore Congestion Map 계산\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions, cell_area)\n",
        "\n",
        "        # 시각화\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(moore_congestion_map, extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], origin='lower', cmap='hot')\n",
        "        plt.colorbar(label=\"Congestion (agents/m²)\")\n",
        "        plt.title(f\"Moore Congestion Map at Time-step {time_step + 1}\")\n",
        "        plt.xlabel(\"X Coordinate\")\n",
        "        plt.ylabel(\"Y Coordinate\")\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa6d33d-3f9b-4c94-8405-e6a406112cf6",
      "metadata": {
        "id": "6fa6d33d-3f9b-4c94-8405-e6a406112cf6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\데이터\"\n",
        "\n",
        "# 격자 데이터 로드\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore 이웃 혼잡도 계산 함수 (혼잡도가 존재하는 셀만 계산)\n",
        "def calculate_moore_congestion(grid, agent_positions, cell_area):\n",
        "    congestion_map = np.zeros_like(grid, dtype=float)  # float 타입으로 초기화\n",
        "\n",
        "    # agent 위치를 grid에 맵핑\n",
        "    for x, y in agent_positions:\n",
        "        if 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:\n",
        "            congestion_map[y, x] += 1\n",
        "\n",
        "    # Moore 이웃 계산 (혼잡도가 존재하는 셀만)\n",
        "    moore_congestion_map = np.zeros_like(congestion_map, dtype=float)  # float 타입으로 초기화\n",
        "    active_cells = np.argwhere(congestion_map > 0)\n",
        "\n",
        "    for y, x in active_cells:\n",
        "        cell_count = 0  # 포함된 셀 개수\n",
        "        for dy in range(-1, 2):\n",
        "            for dx in range(-1, 2):\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < grid.shape[0] and 0 <= nx < grid.shape[1]:\n",
        "                    moore_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "                    cell_count += 1  # 포함된 셀 수 증가\n",
        "        if cell_count > 0:\n",
        "             moore_congestion_map[y, x]/= (cell_area * cell_count)  # \\( \\text{m}^2/\\text{인} \\) 변환\n",
        "\n",
        "    return moore_congestion_map\n",
        "\n",
        "# 시나리오 1 데이터 처리 및 시각화\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_9.xlsx\")\n",
        "\n",
        "try:\n",
        "    # 데이터 로드\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "\n",
        "    # 셀 면적 계산\n",
        "    cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # \\( \\text{m}^2 \\)\n",
        "\n",
        "    # 시각화할 time step 목록\n",
        "    time_steps_to_visualize = [0, 499, 999]  # 0초, 500초, 1000초\n",
        "    for time_step in time_steps_to_visualize:\n",
        "        # agent 위치 복원\n",
        "        agent_positions = [path[time_step] for path in congestion_paths if time_step < len(path)]\n",
        "\n",
        "        # Moore Congestion Map 계산\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions, cell_area)\n",
        "\n",
        "        # 시각화\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(moore_congestion_map, extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], origin='lower', cmap='hot')\n",
        "        plt.colorbar(label=\"Congestion (m²/agent)\")\n",
        "        plt.title(f\"Moore Congestion Map at Time-step {time_step + 1}\")\n",
        "        plt.xlabel(\"X Coordinate\")\n",
        "        plt.ylabel(\"Y Coordinate\")\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcaeeec6-692b-424e-82ee-ddb56e8fe497",
      "metadata": {
        "id": "dcaeeec6-692b-424e-82ee-ddb56e8fe497"
      },
      "outputs": [],
      "source": [
        "# 시나리오 1 데이터 처리 및 시각화\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "try:\n",
        "    # 데이터 로드\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "\n",
        "    # 셀 면적 계산\n",
        "    cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # \\( \\text{m}^2 \\)\n",
        "\n",
        "    # 시각화할 time step 목록\n",
        "    time_steps_to_visualize = [0, 499, 999]  # 0초, 500초, 1000초\n",
        "    for time_step in time_steps_to_visualize:\n",
        "        # agent 위치 복원\n",
        "        agent_positions = [path[time_step] for path in congestion_paths if time_step < len(path)]\n",
        "\n",
        "        # Moore Congestion Map 계산\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions, cell_area)\n",
        "\n",
        "        # 시각화\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(moore_congestion_map, extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], origin='lower', cmap='hot')\n",
        "        plt.colorbar(label=\"Congestion (m²/agent)\")\n",
        "        plt.title(f\"Moore Congestion Map at Time-step {time_step + 1}\")\n",
        "        plt.xlabel(\"X Coordinate\")\n",
        "        plt.ylabel(\"Y Coordinate\")\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993c8e50-3ac5-450c-8dbd-548fb3e7b744",
      "metadata": {
        "id": "993c8e50-3ac5-450c-8dbd-548fb3e7b744"
      },
      "source": [
        "## 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5615f432-423f-455a-8dc7-c8b250273363",
      "metadata": {
        "id": "5615f432-423f-455a-8dc7-c8b250273363"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "\n",
        "# 각 time step에서 최대 혼잡도를 계산\n",
        "max_congestions = [\n",
        "    max(step.values()) if step else 0  # step이 비어있을 경우 0 처리\n",
        "    for step in time_step_congestion\n",
        "]\n",
        "\n",
        "# 시간 축 생성 (1초부터 1000초까지만)\n",
        "time_steps = range(1, 1001)\n",
        "\n",
        "# 1000초까지만 데이터 슬라이싱\n",
        "max_congestions = max_congestions[:1000]\n",
        "\n",
        "# 그래프 그리기\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(time_steps, max_congestions, color='blue', label=\"최대 혼잡도\", alpha=0.8)\n",
        "plt.title(\"각 시간별 최대 혼잡도\", fontsize=14)\n",
        "plt.xlabel(\"시간(초)\", fontsize=12)\n",
        "plt.ylabel(\"최대 혼잡도\", fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c3f517-34ce-4983-bc6a-5aaea000dea0",
      "metadata": {
        "id": "81c3f517-34ce-4983-bc6a-5aaea000dea0"
      },
      "source": [
        "시나리오 8\n",
        "\n",
        "\n",
        "![download.png](attachment:631f963d-ebe5-4442-9579-febb2cd2857d.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489bc266-3585-4675-9909-26ce04a99a3b",
      "metadata": {
        "id": "489bc266-3585-4675-9909-26ce04a99a3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "\n",
        "# 데이터 복구: str -> list 변환 함수\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# 시간대별 격자 혼잡도 계산 함수\n",
        "def calculate_congestion_per_time_step(congestion_paths):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        positions_at_t = []\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                position = path[t]\n",
        "                if isinstance(position, tuple):  # 데이터가 tuple인지 확인\n",
        "                    positions_at_t.append(position)\n",
        "\n",
        "        position_counts = Counter(positions_at_t)\n",
        "        time_step_congestion.append(position_counts)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# 시뮬레이션 파일 처리\n",
        "simulation_files = [os.path.join(base_path, f\"simulation_results_{i}.xlsx\") for i in range(1, 10)]\n",
        "all_simulations = []\n",
        "\n",
        "for i, file in enumerate(simulation_files, start=1):\n",
        "    try:\n",
        "        # 데이터 로드\n",
        "        data = pd.read_excel(file, sheet_name=\"보행자 데이터\", usecols=[\"혼잡 경로\"])\n",
        "        congestion_paths = [parse_congestion_path(path) for path in data[\"혼잡 경로\"].tolist()]\n",
        "        time_step_congestion = calculate_congestion_per_time_step(congestion_paths)\n",
        "\n",
        "        # 각 time step의 최대 혼잡도 계산\n",
        "        max_congestions = [\n",
        "            max(step.values()) if step else 0  # 비어있으면 0 처리\n",
        "            for step in time_step_congestion\n",
        "        ]\n",
        "\n",
        "        # 길이 고정: 1000개의 time step으로 맞추기\n",
        "        if len(max_congestions) < 1000:\n",
        "            max_congestions.extend([max_congestions[-1]] * (1000 - len(max_congestions)))\n",
        "        else:\n",
        "            max_congestions = max_congestions[:1000]\n",
        "\n",
        "        time_steps = list(range(1, 1001))\n",
        "\n",
        "        # DataFrame 생성\n",
        "        df = pd.DataFrame({\n",
        "            \"Time Step\": time_steps,\n",
        "            f\"대피소 인지 비율 {100 - i * 10}%\": max_congestions\n",
        "        })\n",
        "\n",
        "        if len(all_simulations) == 0:\n",
        "            all_simulations = df\n",
        "        else:\n",
        "            all_simulations = pd.merge(all_simulations, df, on=\"Time Step\", how=\"outer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# 하나의 시트로 저장\n",
        "output_file = os.path.join(base_path, \"simulation_results_combined.xlsx\")\n",
        "all_simulations.to_excel(output_file, index=False)\n",
        "print(f\"모든 시뮬레이션 결과가 {output_file}에 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef69553-3578-4ae0-9757-1c957ac2eadb",
      "metadata": {
        "id": "aef69553-3578-4ae0-9757-1c957ac2eadb"
      },
      "outputs": [],
      "source": [
        "simulation_congestion_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119bda48-0770-4ab3-bbec-00a3c91dc3cb",
      "metadata": {
        "id": "119bda48-0770-4ab3-bbec-00a3c91dc3cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aae4ada-cca9-4b99-82f2-a84a4ee8047c",
      "metadata": {
        "id": "3aae4ada-cca9-4b99-82f2-a84a4ee8047c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57cef978-1941-4f44-bdfc-0af5a6adaabb",
      "metadata": {
        "id": "57cef978-1941-4f44-bdfc-0af5a6adaabb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4a43aa1b-5975-4ee4-b2ff-ee83d8aea471",
      "metadata": {
        "id": "4a43aa1b-5975-4ee4-b2ff-ee83d8aea471"
      },
      "source": [
        "# 그래프 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d48315-9145-4669-a4e7-029194e13fb8",
      "metadata": {
        "id": "b7d48315-9145-4669-a4e7-029194e13fb8"
      },
      "outputs": [],
      "source": [
        "# 그래프: 대피소 인지에 따른 누적 혼잡도 및 동적 혼잡도 변화\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 경로 설정\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\"\n",
        "simulation_files = [os.path.join(base_path, f\"simulation_results_{i}.xlsx\") for i in range(1, 10)]\n",
        "\n",
        "# Exploratory 비율\n",
        "exploratory_ratios = [0.1 * i for i in range(1, 10)]\n",
        "\n",
        "# 결과 저장 변수\n",
        "max_accumulated_congestions = []\n",
        "mean_accumulated_congestions = []\n",
        "max_dynamic_congestions = []\n",
        "mean_dynamic_congestions = []\n",
        "\n",
        "# 데이터 추출\n",
        "for file in simulation_files:\n",
        "    data = pd.ExcelFile(file)\n",
        "\n",
        "    # 누적 혼잡도 통계\n",
        "    accumulated_stats = pd.read_excel(data, sheet_name=\"누적 혼잡도 통계\", header=1)\n",
        "    max_accumulated_congestions.append(accumulated_stats.loc[accumulated_stats.iloc[:, 0] == 'max', accumulated_stats.columns[1]].values[0])\n",
        "    mean_accumulated_congestions.append(accumulated_stats.loc[accumulated_stats.iloc[:, 0] == 'mean', accumulated_stats.columns[1]].values[0])\n",
        "\n",
        "    # 동적 혼잡도 통계\n",
        "    dynamic_stats = pd.read_excel(data, sheet_name=\"동적 혼잡도 통계\", header=1)\n",
        "    max_dynamic_congestions.append(dynamic_stats.loc[dynamic_stats.iloc[:, 0] == 'max', dynamic_stats.columns[1]].values[0])\n",
        "    mean_dynamic_congestions.append(dynamic_stats.loc[dynamic_stats.iloc[:, 0] == 'mean', dynamic_stats.columns[1]].values[0])\n",
        "\n",
        "    # 전체 통계에서 평균 혼잡도 추출\n",
        "    overall_stats = pd.read_excel(data, sheet_name=\"전체 통계\")\n",
        "    avg_congestion = overall_stats.iloc[0, 2]  # 평균 혼잡도가 3번째 열에 있다고 가정\n",
        "    avg_congestions.append(avg_congestion)\n",
        "\n",
        "# 그래프 생성: 누적 혼잡도\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, max_accumulated_congestions, marker='o', label='누적 혼잡도 최댓값', color='red')\n",
        "plt.plot(exploratory_ratios, mean_accumulated_congestions, marker='s', label='누적 혼잡도 평균값', color='blue')\n",
        "plt.title(\"대피소 인지에 따른 누적 혼잡도 변화\", fontsize=14)\n",
        "plt.xlabel(\"대피소 인지 못한 대피자 비율\", fontsize=12)\n",
        "plt.ylabel(\"누적 혼잡도\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(base_path, \"accumulated_congestion_vs_exploratory.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 그래프 생성: 동적 혼잡도\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, max_dynamic_congestions, marker='o', label='동적 혼잡도 최댓값', color='green')\n",
        "plt.plot(exploratory_ratios, mean_dynamic_congestions, marker='s', label='동적 혼잡도 평균값', color='orange')\n",
        "plt.title(\"대피소 인지에 따른 동적 혼잡도 변화\", fontsize=14)\n",
        "plt.xlabel(\"대피소 인지 못한 대피자 비율\", fontsize=12)\n",
        "plt.ylabel(\"동적 혼잡도\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(base_path, \"dynamic_congestion_vs_exploratory.png\"), dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cda82e-9ca2-4635-bc9c-381db57226cb",
      "metadata": {
        "id": "c9cda82e-9ca2-4635-bc9c-381db57226cb"
      },
      "outputs": [],
      "source": [
        "# 그래프 생성: 누적 혼잡도\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, mean_accumulated_congestions, marker='s', label='누적 혼잡도 평균값', color='blue')\n",
        "plt.title(\"대피소 인지에 따른 누적 혼잡도 변화\", fontsize=14)\n",
        "plt.xlabel(\"대피소 인지 못한 대피자 비율\", fontsize=12)\n",
        "plt.ylabel(\"누적 혼잡도\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab379f75-a058-43e6-bd38-ef9319d66948",
      "metadata": {
        "id": "ab379f75-a058-43e6-bd38-ef9319d66948"
      },
      "outputs": [],
      "source": [
        "# 그래프 및 데이터 저장을 위한 변수 추가\n",
        "avg_congestions = []  # 전체 통계 시트의 평균 혼잡도 저장\n",
        "\n",
        "# 데이터 추출 (기존 코드에 이어짐)\n",
        "for file in simulation_files:\n",
        "    data = pd.ExcelFile(file)\n",
        "\n",
        "    # 전체 통계에서 평균 혼잡도 추출\n",
        "    overall_stats = pd.read_excel(data, sheet_name=\"전체 통계\")\n",
        "    avg_congestion = overall_stats.loc[0, '평균 혼잡도']\n",
        "    avg_congestions.append(avg_congestion)\n",
        "\n",
        "# 그래프 생성: 평균 혼잡도\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, avg_congestions, marker='o', label='평균 혼잡도', color='purple')\n",
        "plt.title(\"대피소 인지에 따른 평균 혼잡도 변화\", fontsize=14)\n",
        "plt.xlabel(\"대피소 인지 못한 Exploratory 비율\", fontsize=12)\n",
        "plt.ylabel(\"평균 혼잡도\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(base_path, \"average_congestion_vs_exploratory.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 데이터 저장: 혼잡도 및 평균 혼잡도 엑셀로 저장\n",
        "output_data = pd.DataFrame({\n",
        "    'Exploratory 비율': exploratory_ratios,\n",
        "    '누적 혼잡도 최댓값': max_accumulated_congestions,\n",
        "    '누적 혼잡도 평균값': mean_accumulated_congestions,\n",
        "    '동적 혼잡도 최댓값': max_dynamic_congestions,\n",
        "    '동적 혼잡도 평균값': mean_dynamic_congestions,\n",
        "    '평균 혼잡도': avg_congestions\n",
        "})\n",
        "\n",
        "# 결과 저장 경로 설정\n",
        "output_file = os.path.join(base_path, \"congestion_analysis_results.xlsx\")\n",
        "\n",
        "# 엑셀로 저장\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    output_data.to_excel(writer, index=False, sheet_name='혼잡도 분석 결과')\n",
        "\n",
        "print(f\"결과가 다음 경로에 저장되었습니다: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a0b1ed-0a59-4ea9-95fd-afd59e885886",
      "metadata": {
        "id": "d4a0b1ed-0a59-4ea9-95fd-afd59e885886"
      },
      "outputs": [],
      "source": [
        "overall_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c6e5b9-6709-4806-9939-e3d5e3b78e49",
      "metadata": {
        "id": "91c6e5b9-6709-4806-9939-e3d5e3b78e49"
      },
      "outputs": [],
      "source": [
        "# 데이터 추출\n",
        "success_rates=[]\n",
        "for file in simulation_files:\n",
        "    data = pd.ExcelFile(file)\n",
        "# 대피 성공률 추출\n",
        "    overall_stats = pd.read_excel(data, sheet_name=\"전체 통계\")\n",
        "    success_rate = overall_stats.loc[0, '대피 성공률(%)']\n",
        "    success_rates.append(success_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2506d25b-1e19-43ba-abd2-9c129f1088e5",
      "metadata": {
        "id": "2506d25b-1e19-43ba-abd2-9c129f1088e5"
      },
      "outputs": [],
      "source": [
        "# 그래프 2: 대피소 인지 비율에 따른 대피 성공률\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, success_rates, marker='o', color='green', label='대피 성공률')\n",
        "plt.title(\"대피소 인지 비율에 따른 대피 성공률\", fontsize=14)\n",
        "plt.xlabel(\"대피소 인지 못한 Exploratory 비율\", fontsize=12)\n",
        "plt.ylabel(\"대피 성공률\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48b5a61-5ea6-4ec1-8810-2fb741f0c7ac",
      "metadata": {
        "id": "d48b5a61-5ea6-4ec1-8810-2fb741f0c7ac"
      },
      "outputs": [],
      "source": [
        "success_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23215ff-1272-4c12-b9dc-ea936d0831b4",
      "metadata": {
        "id": "b23215ff-1272-4c12-b9dc-ea936d0831b4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b6039f-49a7-4d9a-9df3-7251609cf8ee",
      "metadata": {
        "id": "71b6039f-49a7-4d9a-9df3-7251609cf8ee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c2e725-0da2-4a71-b4b8-299ce85fb8d5",
      "metadata": {
        "id": "a1c2e725-0da2-4a71-b4b8-299ce85fb8d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce99f2b-78e0-4fd6-8736-a0c653eaf9ad",
      "metadata": {
        "id": "7ce99f2b-78e0-4fd6-8736-a0c653eaf9ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd5ed25-8d42-4d68-9c79-15c0da196f50",
      "metadata": {
        "id": "4bd5ed25-8d42-4d68-9c79-15c0da196f50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816ba33c-7b83-4b46-8fe8-47bc910de2e2",
      "metadata": {
        "id": "816ba33c-7b83-4b46-8fe8-47bc910de2e2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbb8e8c-cbac-48f4-b161-2c8eadc5e365",
      "metadata": {
        "id": "afbb8e8c-cbac-48f4-b161-2c8eadc5e365"
      },
      "outputs": [],
      "source": [
        "1회 하던떄 (반복 안할떄)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29416768-53fb-4ad8-8a50-c7d7e9abf989",
      "metadata": {
        "id": "29416768-53fb-4ad8-8a50-c7d7e9abf989"
      },
      "outputs": [],
      "source": [
        "# 보행자 생성 파라미터 설정\n",
        "total_agents = 1000\n",
        "special_area_ratio = 0.3\n",
        "agent_type_ratios = {'노인/어린이': 0.29, '중장년': 0.34, '청소년/청년': 0.37, '장애인': 0.07}\n",
        "agent_types = {'노인/어린이': 1.0, '중장년': 1.3, '청소년/청년': 1.9, '장애인': 0.71}\n",
        "behaviors = {'knows_specific': 0.3, 'knows_all': 0.4, 'exploratory': 0.3}\n",
        "# 보행자 생성\n",
        "pedestrians = generate_pedestrians(\n",
        "    total_agents, agent_type_ratios, agent_types, behaviors,\n",
        "    special_area_cells, general_area_cells, evacuation_target_area_cells,\n",
        "    x_coords, y_coords, special_area_ratio\n",
        ")\n",
        "\n",
        "# 생성된 보행자 수 확인\n",
        "print(f\"Total pedestrians generated: {len(pedestrians)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b17ad5-d8dc-4a33-8bfb-f9cee50b3e39",
      "metadata": {
        "id": "30b17ad5-d8dc-4a33-8bfb-f9cee50b3e39",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dynamic_congestion_grid, accumulated_congestion_grid, evacuation_details, max_congestion_per_step= run_simulation(grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords, area_gdf, buildings_gdf, 100, evacuation_target_area_gdf, output_dir, flood_cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a008f16-7e44-4e00-8e16-f9a1dca98269",
      "metadata": {
        "id": "2a008f16-7e44-4e00-8e16-f9a1dca98269"
      },
      "outputs": [],
      "source": [
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, accumulated_congestion_grid, x_coords, y_coords)\n",
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, dynamic_congestion_grid, x_coords, y_coords)\n",
        "save_results_to_excel(\n",
        "    pedestrians=pedestrians,\n",
        "    filename=r'C:\\Users\\doohu\\Desktop\\대학원\\대피 시뮬레이션_최종코드\\결과\\결과1.xlsx',\n",
        "    accumulated_congestion_grid=accumulated_congestion_grid,\n",
        "    max_congestion_per_step=max_congestion_per_step\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ebcdea-20d5-4122-a226-981e403c322a",
      "metadata": {
        "id": "d8ebcdea-20d5-4122-a226-981e403c322a"
      },
      "outputs": [],
      "source": [
        "\n",
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, accumulated_congestion_grid, x_coords, y_coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32914476-3678-46e0-9a28-ec241d4e416d",
      "metadata": {
        "id": "32914476-3678-46e0-9a28-ec241d4e416d"
      },
      "outputs": [],
      "source": [
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, dynamic_congestion_grid, x_coords, y_coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b05377-673b-4545-9aa4-33b874274038",
      "metadata": {
        "id": "48b05377-673b-4545-9aa4-33b874274038"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf8d0e6-6212-49ed-b935-cde955cafbea",
      "metadata": {
        "id": "7bf8d0e6-6212-49ed-b935-cde955cafbea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a97052-bb09-4172-b4e0-4f3a046da83b",
      "metadata": {
        "id": "61a97052-bb09-4172-b4e0-4f3a046da83b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4bd9692-a1ea-4cfc-82b6-3d39e9f3048a",
      "metadata": {
        "id": "b4bd9692-a1ea-4cfc-82b6-3d39e9f3048a"
      },
      "outputs": [],
      "source": [
        "ㅁㄴㅁㄴㅁㄴ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abd8ef0-04e4-43f7-bed7-5e3c37cf08b1",
      "metadata": {
        "id": "3abd8ef0-04e4-43f7-bed7-5e3c37cf08b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b1e27a-68e1-4ecf-9273-6fe343eed424",
      "metadata": {
        "id": "11b1e27a-68e1-4ecf-9273-6fe343eed424"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09460eb-761f-48bc-8328-cb4aa8341338",
      "metadata": {
        "id": "e09460eb-761f-48bc-8328-cb4aa8341338"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34cf90d1-b2a2-419f-b624-6b901ad21290",
      "metadata": {
        "id": "34cf90d1-b2a2-419f-b624-6b901ad21290"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e2f1a5-dab6-44e2-9259-d21e6f418f9f",
      "metadata": {
        "id": "30e2f1a5-dab6-44e2-9259-d21e6f418f9f"
      },
      "outputs": [],
      "source": [
        "ㅁㄴㅁㄴㅁㄴ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16cf4da-9c1c-4f12-9c5d-4617b9740003",
      "metadata": {
        "id": "d16cf4da-9c1c-4f12-9c5d-4617b9740003"
      },
      "outputs": [],
      "source": [
        "from openpyxl import Workbook\n",
        "\n",
        "def save_evacuations_to_excel(pedestrians, evacuation_target_area_gdf, x_coords, y_coords, filename):\n",
        "    \"\"\"\n",
        "    보행자 시뮬레이션 결과를 Excel 파일로 저장하며, 다양한 통계 시트를 포함합니다.\n",
        "\n",
        "    Parameters:\n",
        "        pedestrians (list): 보행자 객체 리스트\n",
        "        evacuation_target_area_gdf (GeoDataFrame): 대피소 영역 GeoDataFrame\n",
        "        x_coords, y_coords (numpy.ndarray): 격자 좌표를 EPSG:5181로 변환하는 배열\n",
        "        filename (str): 저장할 Excel 파일 이름\n",
        "    \"\"\"\n",
        "    # 결과를 저장할 데이터 리스트\n",
        "    results = []\n",
        "    for pedestrian in pedestrians:\n",
        "        escape_time = round(pedestrian.total_distance / pedestrian.speed, 2) if pedestrian.speed > 0 else None\n",
        "\n",
        "        result = {\n",
        "            \"ID\": pedestrian.id,\n",
        "            \"현재 위치\": pedestrian.position,\n",
        "            \"속도\": pedestrian.speed,\n",
        "            \"행동 유형\": pedestrian.behavior,\n",
        "            \"보행자 타입\": pedestrian.type,\n",
        "            \"시장 지역 시작 여부\": pedestrian.special_area,\n",
        "            \"대피소 인지 여부\": pedestrian.knows_target,\n",
        "            \"대피 목표\": pedestrian.goal,\n",
        "            \"대피 성공 여부\": pedestrian.goal_reached,\n",
        "            \"실제 경로\": pedestrian.real_path,\n",
        "            \"총 이동 거리(미터)\": pedestrian.total_distance,\n",
        "            \"소요 시간(초)\": escape_time\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # DataFrame 생성\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Sheet 1: 각 보행자별 상세 결과\n",
        "    sheet1 = df\n",
        "\n",
        "    # Sheet 2: 대피 유형별 대피 현황\n",
        "    behavior_stats = df.groupby(\"행동 유형\").agg(\n",
        "        총_보행자수=(\"ID\", \"count\"),\n",
        "        대피_성공자수=(\"대피 성공 여부\", \"sum\"),\n",
        "        대피_성공률_퍼센트=(\"대피 성공 여부\", lambda x: round(x.mean() * 100, 2)),\n",
        "        평균_소요시간=(\"소요 시간(초)\", \"mean\"),\n",
        "        평균_이동거리=(\"총 이동 거리(미터)\", \"mean\")\n",
        "    )\n",
        "    behavior_stats.index.name = \"행동 유형\"\n",
        "\n",
        "    # Sheet 3: 보행자 타입별 대피 현황\n",
        "    agent_type_stats = df.groupby(\"보행자 타입\").agg(\n",
        "        총_보행자수=(\"ID\", \"count\"),\n",
        "        대피_성공자수=(\"대피 성공 여부\", \"sum\"),\n",
        "        대피_성공률_퍼센트=(\"대피 성공 여부\", lambda x: round(x.mean() * 100, 2)),\n",
        "        평균_소요시간=(\"소요 시간(초)\", \"mean\"),\n",
        "        평균_이동거리=(\"총 이동 거리(미터)\", \"mean\")\n",
        "    )\n",
        "    agent_type_stats.index.name = \"보행자 타입\"\n",
        "\n",
        "    # Sheet 4: 대피소별 대피 현황\n",
        "    evacuation_data = []\n",
        "    valid_goals = df[df[\"대피 목표\"].notnull()]\n",
        "\n",
        "    geometry = []\n",
        "    for goal in valid_goals[\"대피 목표\"]:\n",
        "        if isinstance(goal, tuple) and 0 <= goal[1] < len(x_coords) and 0 <= goal[0] < len(y_coords):\n",
        "            geometry.append(Point(x_coords[goal[1]], y_coords[goal[0]]))\n",
        "        else:\n",
        "            geometry.append(None)\n",
        "\n",
        "    pedestrian_goals = gpd.GeoDataFrame(\n",
        "        valid_goals,\n",
        "        geometry=geometry,\n",
        "        crs=\"EPSG:5181\"\n",
        "    ).dropna(subset=[\"geometry\"])\n",
        "\n",
        "    for idx, shelter in evacuation_target_area_gdf.iterrows():\n",
        "        shelter_name = f\"{shelter.get('A24', '')}{shelter.get('A25', '')}\"\n",
        "        shelter_geometry = shelter.geometry\n",
        "        reached_pedestrians = pedestrian_goals[pedestrian_goals.geometry.within(shelter_geometry)]\n",
        "\n",
        "        evacuation_data.append({\n",
        "            \"대피소 이름\": shelter_name,\n",
        "            \"대피소 좌표\": (shelter_geometry.centroid.x, shelter_geometry.centroid.y),\n",
        "            \"도착한 보행자 수\": len(reached_pedestrians),\n",
        "            \"평균 소요 시간(초)\": reached_pedestrians[\"소요 시간(초)\"].mean() if len(reached_pedestrians) > 0 else None,\n",
        "            \"평균 이동 거리(미터)\": reached_pedestrians[\"총 이동 거리(미터)\"].mean() if len(reached_pedestrians) > 0 else None,\n",
        "            \"보행자 타입별 도착 수\": reached_pedestrians[\"보행자 타입\"].value_counts().to_dict(),\n",
        "            \"행동 유형별 도착 수\": reached_pedestrians[\"행동 유형\"].value_counts().to_dict()\n",
        "        })\n",
        "\n",
        "    shelter_stats = pd.DataFrame(evacuation_data)\n",
        "\n",
        "    # Excel 파일로 저장\n",
        "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
        "        sheet1.to_excel(writer, sheet_name=\"보행자 상세 결과\", index=False)\n",
        "        behavior_stats.to_excel(writer, sheet_name=\"대피 유형별 현황\")\n",
        "        agent_type_stats.to_excel(writer, sheet_name=\"보행자 타입별 현황\")\n",
        "        shelter_stats.to_excel(writer, sheet_name=\"대피소별 현황\", index=False)\n",
        "\n",
        "    print(f\"Evacuation results saved to '{filename}' with 4 sheets.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b819f88e-d2f2-4905-806f-c14fde26c289",
      "metadata": {
        "id": "b819f88e-d2f2-4905-806f-c14fde26c289"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "badcafd6-7773-4c23-94b5-a0b13f43874c",
      "metadata": {
        "id": "badcafd6-7773-4c23-94b5-a0b13f43874c"
      },
      "outputs": [],
      "source": [
        "save_evacuations_to_excel(pedestrians, r\"C:\\Users\\doohu\\Desktop\\digital_twin\\results\\evacuation_results4.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c788dc22-1359-4539-b60e-ea46acad35cc",
      "metadata": {
        "id": "c788dc22-1359-4539-b60e-ea46acad35cc"
      },
      "outputs": [],
      "source": [
        "filename = r\"C:\\Users\\doohu\\Desktop\\digital_twin\\results\\evacuation_results3.xlsx\"\n",
        "# Call the function\n",
        "save_evacuations_to_excel(\n",
        "    pedestrians=pedestrians,\n",
        "    evacuation_target_area_gdf=evacuation_target_area_gdf,\n",
        "    x_coords=x_coords,\n",
        "    y_coords=y_coords,\n",
        "    filename=filename\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e4241-5c77-492e-b0bd-bade331935f2",
      "metadata": {
        "id": "5b3e4241-5c77-492e-b0bd-bade331935f2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import re\n",
        "\n",
        "# 기본 경로 설정\n",
        "base_dir = r\"C:\\Users\\doohu\\Desktop\\digital_twin\\20250106\"\n",
        "output_dir = os.path.join(base_dir, \"videos\")\n",
        "\n",
        "# 하위 경로 목록\n",
        "sub_dirs = [\"by_behavior\", \"by_congestion\", \"by_type\"]\n",
        "\n",
        "# 정규 표현식으로 step 번호 추출\n",
        "step_pattern = re.compile(r\"step_(\\d+)\")\n",
        "\n",
        "# 비디오 생성 함수\n",
        "def create_video_from_images(image_folder, output_video_path, frame_rate=24.0):\n",
        "    if not os.path.exists(image_folder):\n",
        "        print(f\"이미지 폴더가 존재하지 않습니다: {image_folder}\")\n",
        "        return\n",
        "\n",
        "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\") or img.endswith(\".jpg\")]\n",
        "\n",
        "    # 이미지 파일에서 step 번호 추출 및 정렬\n",
        "    try:\n",
        "        images.sort(key=lambda x: int(step_pattern.search(x).group(1)) if step_pattern.search(x) else float('inf'))\n",
        "    except Exception as e:\n",
        "        print(f\"이미지 정렬 중 오류 발생: {e}\")\n",
        "        return\n",
        "\n",
        "    if not images:\n",
        "        print(f\"{image_folder}에 이미지가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    # 첫 이미지로 프레임 크기 설정\n",
        "    first_image_path = os.path.join(image_folder, images[0])\n",
        "    print(f\"첫 이미지 경로: {first_image_path}\")\n",
        "\n",
        "    if not os.path.exists(first_image_path):\n",
        "        print(f\"파일이 존재하지 않습니다: {first_image_path}\")\n",
        "        return\n",
        "\n",
        "    frame = cv2.imread(first_image_path)\n",
        "\n",
        "    if frame is None:\n",
        "        print(f\"이미지를 읽지 못했습니다: {first_image_path}\")\n",
        "        return\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "    size = (width, height)\n",
        "\n",
        "    # VideoWriter 객체 생성\n",
        "    os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, size)\n",
        "\n",
        "    # 이미지들을 비디오에 추가\n",
        "    for image in images:\n",
        "        img_path = os.path.join(image_folder, image)\n",
        "        frame = cv2.imread(img_path)\n",
        "        if frame is not None:\n",
        "            out.write(frame)\n",
        "        else:\n",
        "            print(f\"이미지를 읽지 못했습니다: {img_path}\")\n",
        "\n",
        "    out.release()\n",
        "    print(f\"영상 생성 완료: {output_video_path}\")\n",
        "\n",
        "# 각 하위 경로에 대해 영상 생성\n",
        "for sub_dir in sub_dirs:\n",
        "    image_folder = os.path.join(base_dir, sub_dir)\n",
        "    output_video_path = os.path.join(output_dir, f\"{sub_dir}2.avi\")\n",
        "    print(f\"Processing: {sub_dir}\")\n",
        "    create_video_from_images(image_folder, output_video_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}