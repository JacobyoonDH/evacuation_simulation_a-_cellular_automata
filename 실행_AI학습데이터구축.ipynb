{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b5fb96-5019-4007-94dd-a483959cd79b",
      "metadata": {
        "id": "b0b5fb96-5019-4007-94dd-a483959cd79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ffedf69-196a-4799-df90-4599a7550a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting python-dateutil\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: six, certifi, python-dateutil\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "Successfully installed certifi-2025.4.26 python-dateutil-2.9.0.post0 six-1.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "dateutil",
                  "six"
                ]
              },
              "id": "0cce9455f24c4272916ccfd8b91984ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiona==1.9.5\n",
            "  Downloading fiona-1.9.5-cp311-cp311-manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geopandas==0.13.2\n",
            "  Downloading geopandas-0.13.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting shapely==2.0.2\n",
            "  Downloading shapely-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting attrs>=19.2.0 (from fiona==1.9.5)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi (from fiona==1.9.5)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting click~=8.0 (from fiona==1.9.5)\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting click-plugins>=1.0 (from fiona==1.9.5)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cligj>=0.5 (from fiona==1.9.5)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting six (from fiona==1.9.5)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting setuptools (from fiona==1.9.5)\n",
            "  Downloading setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging (from geopandas==0.13.2)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pandas>=1.1.0 (from geopandas==0.13.2)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyproj>=3.0.1 (from geopandas==0.13.2)\n",
            "  Downloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas>=1.1.0->geopandas==0.13.2)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=1.1.0->geopandas==0.13.2)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.1.0->geopandas==0.13.2)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.9.5-cp311-cp311-manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geopandas-0.13.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shapely-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproj-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.4.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz, tzdata, six, setuptools, packaging, numpy, click, certifi, attrs, shapely, python-dateutil, pyproj, cligj, click-plugins, pandas, fiona, geopandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.1.0\n",
            "    Uninstalling shapely-2.1.0:\n",
            "      Successfully uninstalled shapely-2.1.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pyproj\n",
            "    Found existing installation: pyproj 3.7.1\n",
            "    Uninstalling pyproj-3.7.1:\n",
            "      Successfully uninstalled pyproj-3.7.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: geopandas\n",
            "    Found existing installation: geopandas 1.0.1\n",
            "    Uninstalling geopandas-1.0.1:\n",
            "      Successfully uninstalled geopandas-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "langchain-core 0.3.56 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-25.3.0 certifi-2025.4.26 click-8.2.0 click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.5 geopandas-0.13.2 numpy-1.26.4 packaging-25.0 pandas-2.2.3 pyproj-3.7.1 python-dateutil-2.9.0.post0 pytz-2025.2 setuptools-80.4.0 shapely-2.0.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi",
                  "dateutil",
                  "six"
                ]
              },
              "id": "ba54c12f7bab41de8082ee4624e02db4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì •í™•íˆ ë§ì¶° ì¬ì„¤ì¹˜\n",
        "!pip install --upgrade --force-reinstall certifi six python-dateutil\n",
        "\n",
        "!pip install --force-reinstall numpy==1.26.4 fiona==1.9.5 geopandas==0.13.2 shapely==2.0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import certifi, six, dateutil, numpy, fiona, geopandas, shapely\n",
        "\n",
        "print(\"certifi:\", certifi.__version__)\n",
        "print(\"six:\", six.__version__)\n",
        "print(\"python-dateutil:\", dateutil.__version__)\n",
        "print(\"numpy:\", numpy.__version__)\n",
        "print(\"fiona:\", fiona.__version__)\n",
        "print(\"geopandas:\", geopandas.__version__)\n",
        "print(\"shapely:\", shapely.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o40B37Sl1R7C",
        "outputId": "44c4ff1e-0bd9-42cf-e69d-f13feaa9c6a9"
      },
      "id": "o40B37Sl1R7C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "certifi: 2025.04.26\n",
            "six: 1.17.0\n",
            "python-dateutil: 2.9.0.post0\n",
            "numpy: 1.26.4\n",
            "fiona: 1.9.5\n",
            "geopandas: 0.13.2\n",
            "shapely: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sUk5VERbAhjo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUk5VERbAhjo",
        "outputId": "b44d0122-b1b2-4981-f566-15051b927e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ.zip\n",
            "  inflating: /content/sim_code/aa.py  \n",
            "  inflating: /content/sim_code/grid_utils.py  \n",
            "  inflating: /content/sim_code/main.py  \n",
            "  inflating: /content/sim_code/pedestrian.py  \n",
            "  inflating: /content/sim_code/pedestrian_utils.py  \n",
            "  inflating: /content/sim_code/results_utils.py  \n",
            "  inflating: /content/sim_code/simulation.py  \n",
            "  inflating: /content/sim_code/simulation_itter.py  \n",
            "  inflating: /content/sim_code/visualization.py  \n",
            "  inflating: /content/sim_code/ì‹¤í–‰_AIí•™ìŠµë°ì´í„°êµ¬ì¶•.ipynb  \n",
            "Archive:  /content/drive/MyDrive/ë°ì´í„°.zip\n",
            "  inflating: /content/sim_data/agentìƒì„±ìš©.cpg  \n",
            "  inflating: /content/sim_data/agentìƒì„±ìš©.dbf  \n",
            "  inflating: /content/sim_data/agentìƒì„±ìš©.prj  \n",
            "  inflating: /content/sim_data/agentìƒì„±ìš©.shp  \n",
            "  inflating: /content/sim_data/agentìƒì„±ìš©.shx  \n",
            "  inflating: /content/sim_data/area_cells_cache.pkl  \n",
            "  inflating: /content/sim_data/flood_cells_grid.pkl  \n",
            "  inflating: /content/sim_data/grid.npy  \n",
            "  inflating: /content/sim_data/x_coords.csv  \n",
            "  inflating: /content/sim_data/y_coords.csv  \n",
            "  inflating: /content/sim_data/ëŒ€ìƒì§€_ê±´ë¬¼.cpg  \n",
            "  inflating: /content/sim_data/ëŒ€ìƒì§€_ê±´ë¬¼.dbf  \n",
            "  inflating: /content/sim_data/ëŒ€ìƒì§€_ê±´ë¬¼.prj  \n",
            "  inflating: /content/sim_data/ëŒ€ìƒì§€_ê±´ë¬¼.shp  \n",
            "  inflating: /content/sim_data/ëŒ€ìƒì§€_ê±´ë¬¼.shx  \n",
            " extracting: /content/sim_data/ëŒ€í”¼ì†Œ200më²„í¼.cpg  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ200më²„í¼.dbf  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ200më²„í¼.prj  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ200më²„í¼.shp  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ200më²„í¼.shx  \n",
            " extracting: /content/sim_data/ëŒ€í”¼ì†Œ_shp.cpg  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ_shp.dbf  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ_shp.prj  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ_shp.shp  \n",
            "  inflating: /content/sim_data/ëŒ€í”¼ì†Œ_shp.shx  \n",
            "  inflating: /content/sim_data/ì‹œì¥ì˜ì—­.cpg  \n",
            "  inflating: /content/sim_data/ì‹œì¥ì˜ì—­.dbf  \n",
            "  inflating: /content/sim_data/ì‹œì¥ì˜ì—­.prj  \n",
            "  inflating: /content/sim_data/ì‹œì¥ì˜ì—­.shp  \n",
            "  inflating: /content/sim_data/ì‹œì¥ì˜ì—­.shx  \n",
            " extracting: /content/sim_data/ì‹ ê·œëŒ€ìƒì§€_ë„ë³´ì´ë™ê°€ëŠ¥ì§€ì—­.cpg  \n",
            "  inflating: /content/sim_data/ì‹ ê·œëŒ€ìƒì§€_ë„ë³´ì´ë™ê°€ëŠ¥ì§€ì—­.dbf  \n",
            "  inflating: /content/sim_data/ì‹ ê·œëŒ€ìƒì§€_ë„ë³´ì´ë™ê°€ëŠ¥ì§€ì—­.prj  \n",
            "  inflating: /content/sim_data/ì‹ ê·œëŒ€ìƒì§€_ë„ë³´ì´ë™ê°€ëŠ¥ì§€ì—­.shp  \n",
            "  inflating: /content/sim_data/ì‹ ê·œëŒ€ìƒì§€_ë„ë³´ì´ë™ê°€ëŠ¥ì§€ì—­.shx  \n",
            " extracting: /content/sim_data/ì¹¨ìˆ˜ì§€ì—­.cpg  \n",
            "  inflating: /content/sim_data/ì¹¨ìˆ˜ì§€ì—­.dbf  \n",
            "  inflating: /content/sim_data/ì¹¨ìˆ˜ì§€ì—­.prj  \n",
            "  inflating: /content/sim_data/ì¹¨ìˆ˜ì§€ì—­.shp  \n",
            "  inflating: /content/sim_data/ì¹¨ìˆ˜ì§€ì—­.shx  \n"
          ]
        }
      ],
      "source": [
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ì••ì¶• í•´ì œ\n",
        "!unzip \"/content/drive/MyDrive/ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ.zip\" -d /content/sim_code\n",
        "!unzip \"/content/drive/MyDrive/ë°ì´í„°.zip\" -d /content/sim_data\n",
        "\n",
        "# ì½”ë“œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ path ì¶”ê°€\n",
        "import sys\n",
        "sys.path.append('/content/sim_code')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GivmqS1LA0ho",
      "metadata": {
        "id": "GivmqS1LA0ho"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/sim_code')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c21cf07-d57d-40a8-99f4-2cb8400d1911",
      "metadata": {
        "id": "5c21cf07-d57d-40a8-99f4-2cb8400d1911"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import matplotlib.patches as mpatches\n",
        "from grid_utils import *\n",
        "from pedestrian_utils import *\n",
        "from pedestrian import *\n",
        "from simulation import *\n",
        "#from results_utils import *\n",
        "#from visualization import *\n",
        "import numpy as np\n",
        "from simulation import *\n",
        "#from results_utils import *\n",
        "#from visualization import *\n",
        "from pedestrian_utils import *\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FvHA1CroBen4",
      "metadata": {
        "id": "FvHA1CroBen4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd62064-8c96-45e9-ab8a-0107eeec7290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Reprojection complete. All data is now in EPSG:5181.\n"
          ]
        }
      ],
      "source": [
        "plt.rcParams['font.family'] = 'Malgun Gothic'  # Windowsìš© í°íŠ¸, Colabì—ì„œëŠ” ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© ê¶Œì¥\n",
        "\n",
        "# ê²½ë¡œ ë³€ê²½ (Colab ê¸°ì¤€)\n",
        "base_data_path = \"/content/sim_data\"\n",
        "\n",
        "area_shp_path = f\"{base_data_path}/ì‹ ê·œëŒ€ìƒì§€_ë„ë³´ì´ë™ê°€ëŠ¥ì§€ì—­.shp\"\n",
        "buildings_shp_path = f\"{base_data_path}/ëŒ€ìƒì§€_ê±´ë¬¼.shp\"\n",
        "special_area_shp_path = f\"{base_data_path}/ì‹œì¥ì˜ì—­.shp\"\n",
        "evacuation_target_area_shp_path = f\"{base_data_path}/ëŒ€í”¼ì†Œ_shp.shp\"\n",
        "creation_area_path = f\"{base_data_path}/agentìƒì„±ìš©.shp\"\n",
        "grid_path = base_data_path\n",
        "flood_path = f\"{base_data_path}/flood_cells_grid.pkl\"\n",
        "\n",
        "# ì¢Œí‘œê³„ í†µì¼\n",
        "target_crs = \"EPSG:5181\"\n",
        "area_gdf = gpd.read_file(area_shp_path).to_crs(target_crs)\n",
        "buildings_gdf = gpd.read_file(buildings_shp_path).to_crs(target_crs)\n",
        "special_area_gdf = gpd.read_file(special_area_shp_path).to_crs(target_crs)\n",
        "evacuation_target_area_gdf = gpd.read_file(evacuation_target_area_shp_path).to_crs(target_crs)\n",
        "creation_area_gdf = gpd.read_file(creation_area_path).to_crs(target_crs)\n",
        "\n",
        "print(\"âœ… Reprojection complete. All data is now in EPSG:5181.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b669a7-6800-49f7-8f27-c8d5cb257aea",
      "metadata": {
        "id": "97b669a7-6800-49f7-8f27-c8d5cb257aea"
      },
      "outputs": [],
      "source": [
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850900b3-1596-4593-8dd7-e39d6eca260c",
      "metadata": {
        "id": "850900b3-1596-4593-8dd7-e39d6eca260c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd7f908-09cb-4f76-d964-90985477448f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully loaded.\n",
            "Flood cells grid loaded from /content/sim_data/flood_cells_grid.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "def load_area_cells():\n",
        "    if os.path.exists(cache_file):\n",
        "        with open(cache_file, \"rb\") as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "            creation_area_cells = cached_data[\"creation_area_cells\"]\n",
        "            special_area_cells = cached_data[\"special_area_cells\"]\n",
        "            evacuation_target_area_cells = cached_data[\"evacuation_target_area_cells\"]\n",
        "            general_area_cells = creation_area_cells  # general_area_cellsëŠ” creation_area_cellsì™€ ë™ì¼\n",
        "\n",
        "            print(\"Data successfully loaded.\")\n",
        "\n",
        "            return creation_area_cells, special_area_cells, evacuation_target_area_cells, general_area_cells\n",
        "    else:\n",
        "        print(\"Cache file not found. Please generate and save the data first.\")\n",
        "        return None, None, None, None\n",
        "cache_file = \"/content/sim_data/area_cells_cache.pkl\"\n",
        "flood_cells_file_path = \"/content/sim_data/flood_cells_grid.pkl\"\n",
        "\n",
        "creation_area_cells, special_area_cells, evacuation_target_area_cells, general_area_cells = load_area_cells()\n",
        "def load_flood_cells_grid(file_path):\n",
        "    \"\"\"\n",
        "    ì €ì¥ëœ Flood cells gridë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: ë¡œë“œëœ flood cells grid\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"{file_path} does not exist.\")\n",
        "    with open(file_path, 'rb') as f:\n",
        "        flood_cells_grid = pickle.load(f)\n",
        "    print(f\"Flood cells grid loaded from {file_path}\")\n",
        "    return flood_cells_grid\n",
        "# ì˜ˆì œ ì‹¤í–‰\n",
        "# flood_gdf ì •ì˜ í•„ìš”\n",
        "# Flood cells grid ë¡œë“œ\n",
        "loaded_flood_cells_grid = load_flood_cells_grid(flood_cells_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9ab957-9387-4f69-b571-6ea771d0520b",
      "metadata": {
        "id": "2a9ab957-9387-4f69-b571-6ea771d0520b"
      },
      "outputs": [],
      "source": [
        "def initialize_pedestrian_positions(total_agents, special_area_cells, general_area_cells, special_area_ratio):\n",
        "    \"\"\"\n",
        "    Initialize pedestrian positions in special and general areas, excluding evacuation targets.\n",
        "\n",
        "    Parameters:\n",
        "        total_agents (int): Total number of agents to generate.\n",
        "        special_area_cells (list): List of special area cells (excludes evacuation targets).\n",
        "        general_area_cells (list): List of general area cells (excludes evacuation targets).\n",
        "        special_area_ratio (float): Ratio of agents in special areas.\n",
        "\n",
        "    Returns:\n",
        "        list: List of initial positions for each pedestrian.\n",
        "    \"\"\"\n",
        "    # Calculate the number of agents in special and general areas\n",
        "    num_special_area_agents = int(total_agents * special_area_ratio)\n",
        "    num_general_area_agents = total_agents - num_special_area_agents\n",
        "\n",
        "    # Randomly select cells for special and general area agents\n",
        "    selected_special_area_cells = random.sample(special_area_cells, num_special_area_agents)\n",
        "    selected_general_area_cells = random.sample(general_area_cells, num_general_area_agents)\n",
        "\n",
        "    # Combine and shuffle all selected cells\n",
        "    all_selected_cells = selected_special_area_cells + selected_general_area_cells\n",
        "    random.shuffle(all_selected_cells)\n",
        "\n",
        "    return all_selected_cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee93826b-9151-4008-a36f-ecc5c38d9df6",
      "metadata": {
        "id": "ee93826b-9151-4008-a36f-ecc5c38d9df6"
      },
      "outputs": [],
      "source": [
        "def create_pedestrians_with_behaviors(total_agents, agent_type_ratios, agent_types, behaviors, positions):\n",
        "    \"\"\"\n",
        "    Create pedestrians with dynamic properties such as behavior based on predefined positions.\n",
        "\n",
        "    Parameters:\n",
        "        total_agents (int): Total number of agents.\n",
        "        agent_type_ratios (dict): Ratio of different agent types.\n",
        "        agent_types (dict): Speeds associated with agent types.\n",
        "        behaviors (dict): Behavioral patterns for the simulation.\n",
        "        positions (list): Predefined positions for each agent.\n",
        "\n",
        "    Returns:\n",
        "        list: List of pedestrian objects with assigned behaviors and properties.\n",
        "    \"\"\"\n",
        "    # Calculate distribution of agent types\n",
        "    type_distribution = []\n",
        "    for agent_type, ratio in agent_type_ratios.items():\n",
        "        num_type_agents = int(total_agents * ratio)\n",
        "        type_distribution.extend([agent_type] * num_type_agents)\n",
        "    random.shuffle(type_distribution)\n",
        "\n",
        "    # Create pedestrian objects\n",
        "    pedestrians = []\n",
        "    for i, position in enumerate(positions):\n",
        "        if i >= total_agents:\n",
        "            break\n",
        "        agent_type = type_distribution[i]\n",
        "        speed = agent_types[agent_type]\n",
        "        behavior = random.choices(list(behaviors.keys()), weights=behaviors.values(), k=1)[0]\n",
        "        special_area = position in special_area_cells  # Assume special_area_cells is accessible\n",
        "        pedestrian = Pedestrian(position, speed, behavior, agent_type, special_area, id=i)\n",
        "        pedestrians.append(pedestrian)\n",
        "\n",
        "    return pedestrians\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0502c567-efe1-4ff9-bc77-bb67ab1eaec9",
      "metadata": {
        "id": "0502c567-efe1-4ff9-bc77-bb67ab1eaec9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def run_simulation_scenario_for_debug_and_save(special_area_ratio, initial_positions):\n",
        "    total_agents = 1000\n",
        "    agent_type_ratios = {'ë…¸ì¸/ì–´ë¦°ì´': 0.29, 'ì¤‘ì¥ë…„': 0.34, 'ì²­ì†Œë…„/ì²­ë…„': 0.37, 'ì¥ì• ì¸': 0.07}\n",
        "    agent_types = {'ë…¸ì¸/ì–´ë¦°ì´': 1.0, 'ì¤‘ì¥ë…„': 1.3, 'ì²­ì†Œë…„/ì²­ë…„': 1.9, 'ì¥ì• ì¸': 0.71}\n",
        "\n",
        "    shelter_knowledge_probabilities = [0.3, 0.6, 0.9]\n",
        "    exploratory_ratios = [0.1, 0.4, 0.7]\n",
        "    knows_ratios = [round((1 - exploratory) / 2, 2) for exploratory in exploratory_ratios]\n",
        "\n",
        "    types_distribution = []\n",
        "    for agent_type, ratio in agent_type_ratios.items():\n",
        "        types_distribution.extend([agent_type] * int(ratio * total_agents))\n",
        "    random.shuffle(types_distribution)\n",
        "\n",
        "    # âœ… ì €ì¥ ê²½ë¡œ ë° ì ‘ë‘ì–´ ì„¤ì • (Colabìš© Google Drive ê²½ë¡œ)\n",
        "    save_dir = \"/content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼\"\n",
        "    prefix = \"ì´ˆê¸°í˜¼ì¡X_\" if special_area_ratio == 0 else \"ì´ˆê¸°í˜¼ì¡O_\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for knowledge_prob in shelter_knowledge_probabilities:\n",
        "        for exploratory, knows in zip(exploratory_ratios, knows_ratios):\n",
        "            behaviors = {'knows_specific': knows, 'knows_all': knows, 'exploratory': exploratory}\n",
        "\n",
        "            for trial in range(1, 11):\n",
        "              file_name = f\"{prefix}ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _{knowledge_prob}_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨{round(knows*2, 2)}_trial{trial}.xlsx\"\n",
        "              file_path = os.path.join(save_dir, file_name)\n",
        "              # âœ… íŒŒì¼ ì¡´ì¬ ì‹œ ìŠ¤í‚µ\n",
        "              if os.path.exists(file_path):\n",
        "                print(f\"âœ… ì´ë¯¸ ì¡´ì¬: {file_name} â†’ ê±´ë„ˆëœ€\")\n",
        "                continue\n",
        "              #print(f\"\\nğŸš€ [ì‹¤í–‰] {file_name}\")\n",
        "\n",
        "              pedestrians = [\n",
        "                    Pedestrian(\n",
        "                        pos, agent_types[agent_type],\n",
        "                        random.choices(list(behaviors.keys()), weights=list(behaviors.values()), k=1)[0],\n",
        "                        agent_type, special_area=False, id=j,\n",
        "                        shelter_knowledge_probability=knowledge_prob\n",
        "                    )\n",
        "                    for j, (pos, agent_type) in enumerate(zip(initial_positions, types_distribution))\n",
        "                ]\n",
        "\n",
        "              run_simulation(\n",
        "                    grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords,\n",
        "                    area_gdf, buildings_gdf, steps=1000, evacuee_area=None,\n",
        "                    save_path=None, flood_cells=loaded_flood_cells_grid\n",
        "                )\n",
        "\n",
        "                # âœ… ì €ì¥ íŒŒì¼ ì´ë¦„ êµ¬ì„± (ë…¼ë¬¸ ì œì¶œ í˜•ì‹ ë°˜ì˜)\n",
        "#                file_name = f\"{prefix}ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _{knowledge_prob}_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨{round(knows*2, 2)}_trial{trial}.xlsx\"\n",
        " #               file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "                # âœ… DataFrame ì €ì¥\n",
        "              data = []\n",
        "              for ped in pedestrians:\n",
        "                  d = vars(ped).copy()\n",
        "                  for k, v in d.items():\n",
        "                      if isinstance(v, (list, tuple, dict)):\n",
        "                          d[k] = str(v)\n",
        "                  data.append(d)\n",
        "\n",
        "              df = pd.DataFrame(data)\n",
        "              df.to_excel(file_path, index=False)\n",
        "              print(f\"âœ… ì €ì¥ ì™„ë£Œ: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0af695-0d11-479a-9392-e14a6e906f40",
      "metadata": {
        "id": "dc0af695-0d11-479a-9392-e14a6e906f40"
      },
      "outputs": [],
      "source": [
        "initial_positions = initialize_pedestrian_positions(\n",
        "    total_agents=1000,\n",
        "    special_area_cells=special_area_cells,\n",
        "    general_area_cells=general_area_cells,\n",
        "    special_area_ratio=0\n",
        ")\n",
        "\n",
        "#run_simulation_scenario_for_debug_and_save(0, initial_positions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def run_single_simulation_scenario(special_area_ratio, initial_positions, knowledge_prob, awareness_ratio, trial):\n",
        "    total_agents = 1000\n",
        "    agent_type_ratios = {'ë…¸ì¸/ì–´ë¦°ì´': 0.29, 'ì¤‘ì¥ë…„': 0.34, 'ì²­ì†Œë…„/ì²­ë…„': 0.37, 'ì¥ì• ì¸': 0.07}\n",
        "    agent_types = {'ë…¸ì¸/ì–´ë¦°ì´': 1.0, 'ì¤‘ì¥ë…„': 1.3, 'ì²­ì†Œë…„/ì²­ë…„': 1.9, 'ì¥ì• ì¸': 0.71}\n",
        "\n",
        "    exploratory = round(1 - awareness_ratio, 2)\n",
        "    knows = round(awareness_ratio / 2, 2)\n",
        "    behaviors = {'knows_specific': knows, 'knows_all': knows, 'exploratory': exploratory}\n",
        "\n",
        "    types_distribution = []\n",
        "    for agent_type, ratio in agent_type_ratios.items():\n",
        "        types_distribution.extend([agent_type] * int(ratio * total_agents))\n",
        "    random.shuffle(types_distribution)\n",
        "\n",
        "    # âœ… ì €ì¥ ê²½ë¡œ ë° íŒŒì¼ëª… ì„¤ì •\n",
        "    save_dir = \"/content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼\"\n",
        "    prefix = \"ì´ˆê¸°í˜¼ì¡X_\" if special_area_ratio == 0 else \"ì´ˆê¸°í˜¼ì¡O_\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    file_name = f\"{prefix}ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _{knowledge_prob}_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨{awareness_ratio}_trial{trial}.xlsx\"\n",
        "    file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"âœ… ì´ë¯¸ ì¡´ì¬: {file_name} â†’ ê±´ë„ˆëœ€\")\n",
        "        return\n",
        "\n",
        "    # âœ… ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
        "    pedestrians = [\n",
        "        Pedestrian(\n",
        "            pos, agent_types[agent_type],\n",
        "            random.choices(list(behaviors.keys()), weights=list(behaviors.values()), k=1)[0],\n",
        "            agent_type, special_area=False, id=j,\n",
        "            shelter_knowledge_probability=knowledge_prob\n",
        "        )\n",
        "        for j, (pos, agent_type) in enumerate(zip(initial_positions, types_distribution))\n",
        "    ]\n",
        "\n",
        "    run_simulation(\n",
        "        grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords,\n",
        "        area_gdf, buildings_gdf, steps=1000, evacuee_area=None,\n",
        "        save_path=None, flood_cells=loaded_flood_cells_grid\n",
        "    )\n",
        "\n",
        "    # âœ… ê²°ê³¼ ì €ì¥\n",
        "    data = []\n",
        "    for ped in pedestrians:\n",
        "        d = vars(ped).copy()\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, (list, tuple, dict)):\n",
        "                d[k] = str(v)\n",
        "        data.append(d)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(file_path, index=False)\n",
        "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {file_path}\")\n"
      ],
      "metadata": {
        "id": "vRzjnyAjsJpE"
      },
      "id": "vRzjnyAjsJpE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ì¬ìˆ˜í–‰ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ (í•„ìš” ì‹œ ì´ ì•„ë˜ì— ì „ì²´ 41ê°œ í•­ëª© ì…ë ¥)\n",
        "# ëˆ„ë½ëœ ì‹œë‚˜ë¦¬ì˜¤ ëª©ë¡ ê¸°ë°˜ìœ¼ë¡œ, ì¬ìˆ˜í–‰ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„± (5ê±´ ì œì™¸)\n",
        "# ì•„ë˜ ë¦¬ìŠ¤íŠ¸ëŠ” ì‚¬ìš©ìê°€ ì´ì „ì— ì œê³µí•œ df_missing_casesì—ì„œ íŒŒìƒë˜ì—ˆê³ , ìƒìœ„ 5ê±´ì€ ì´ë¯¸ ìˆ˜í–‰ë˜ì—ˆìŒ\n",
        "rerun_targets = [\n",
        "    (0.9, 0.6, i) for i in range(1, 10)\n",
        "] + [\n",
        "    (0.9, 0.9, i) for i in range(1, 10)\n",
        "]\n",
        "\n",
        "\n",
        "# âœ… ì‹œë®¬ë ˆì´ì…˜ ë°˜ë³µ ì‹¤í–‰\n",
        "for info_prob, awareness_ratio, trial in rerun_targets:\n",
        "    run_single_simulation_scenario(\n",
        "        special_area_ratio=0,  # ì´ˆê¸°í˜¼ì¡X\n",
        "        initial_positions=initial_positions,\n",
        "        knowledge_prob=info_prob,\n",
        "        awareness_ratio=awareness_ratio,\n",
        "        trial=trial\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8HFLet55sJye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6e5f36-0cb2-42b3-ea54-0e6ce9285b89"
      },
      "id": "8HFLet55sJye",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial1.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial2.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial3.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial4.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial5.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial6.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial7.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial8.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.6_trial9.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial1.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial2.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial3.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial4.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial5.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial6.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial7.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial8.xlsx\n",
            "âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/ì‹œë®¬ë ˆì´ì…˜ê²°ê³¼/ì´ˆê¸°í˜¼ì¡X_ëŒ€í”¼ì†Œì •ë³´ì „íŒŒí™•ë¥ _0.9_ëŒ€í”¼ì†Œì¸ì§€ë¹„ìœ¨0.9_trial9.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3dSOg33sJ1r"
      },
      "id": "m3dSOg33sJ1r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmULrofwsJ4j"
      },
      "id": "UmULrofwsJ4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f122ab25-e520-471b-a7ed-e5eff2791c66",
      "metadata": {
        "id": "f122ab25-e520-471b-a7ed-e5eff2791c66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087c12c3-42ea-455a-aa11-21cabab44294",
      "metadata": {
        "id": "087c12c3-42ea-455a-aa11-21cabab44294"
      },
      "outputs": [],
      "source": [
        "asasasasas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90242c0d-f6bf-4eca-ba55-24a536f9af48",
      "metadata": {
        "id": "90242c0d-f6bf-4eca-ba55-24a536f9af48"
      },
      "source": [
        "### ìµœì¢… ë…¼ë¬¸ìš© ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ba8097-98eb-441c-b614-756327cae691",
      "metadata": {
        "id": "e9ba8097-98eb-441c-b614-756327cae691"
      },
      "outputs": [],
      "source": [
        "ëë‚´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c896c9-4936-4316-afb0-bc9fd2584ad4",
      "metadata": {
        "id": "55c896c9-4936-4316-afb0-bc9fd2584ad4"
      },
      "outputs": [],
      "source": [
        "flood_cells = loaded_flood_cells_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402dc648-7764-4d60-8ba5-c52ec51e26c6",
      "metadata": {
        "id": "402dc648-7764-4d60-8ba5-c52ec51e26c6"
      },
      "outputs": [],
      "source": [
        "# ì´ˆê¸° ì„¤ì •: ë³´í–‰ì ìƒì„± íŒŒë¼ë¯¸í„°\n",
        "total_agents = 1000\n",
        "special_area_ratio = 0.3\n",
        "agent_type_ratios = {'ë…¸ì¸/ì–´ë¦°ì´': 0.29, 'ì¤‘ì¥ë…„': 0.34, 'ì²­ì†Œë…„/ì²­ë…„': 0.37, 'ì¥ì• ì¸': 0.07}\n",
        "agent_types = {'ë…¸ì¸/ì–´ë¦°ì´': 1.0, 'ì¤‘ì¥ë…„': 1.3, 'ì²­ì†Œë…„/ì²­ë…„': 1.9, 'ì¥ì• ì¸': 0.71}\n",
        "\n",
        "# ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •: behaviors ë¹„ìœ¨\n",
        "simulation_count = 9\n",
        "exploratory_ratios = [0.1 * i for i in range(1, 10)]  # 0.1ë¶€í„° 0.9ê¹Œì§€\n",
        "knows_ratios = [round((1 - exploratory) / 2, 2) for exploratory in exploratory_ratios]  # ë‚˜ë¨¸ì§€ ë¹„ìœ¨ì„ ê· ë“±í•˜ê²Œ ë¶„ë°°\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "base_path = r'C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼'\n",
        "\n",
        "# ì‹œë®¬ë ˆì´ì…˜ ë°˜ë³µ\n",
        "for i, (exploratory, knows) in enumerate(zip(exploratory_ratios, knows_ratios), start=1):\n",
        "    # behaviors ì„¤ì •\n",
        "    behaviors = {'knows_specific': knows, 'knows_all': knows, 'exploratory': exploratory}\n",
        "    print(f\"\\nRunning simulation {i} with behaviors: {behaviors}\")\n",
        "\n",
        "    # ë³´í–‰ì ìƒì„±\n",
        "    pedestrians = generate_pedestrians(\n",
        "        total_agents, agent_type_ratios, agent_types, behaviors,\n",
        "        special_area_cells, general_area_cells, evacuation_target_area_cells,\n",
        "        x_coords, y_coords, special_area_ratio\n",
        "    )\n",
        "\n",
        "    # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
        "    dynamic_congestion_grid, accumulated_congestion_grid, evacuation_details, max_congestion_per_step = run_simulation(\n",
        "        grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords,\n",
        "        area_gdf, buildings_gdf, steps=1000, evacuee_area=None, save_path=None, flood_cells=flood_cells\n",
        "    )\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "    excel_filename = os.path.join(base_path, f'simulation_results_{i}.xlsx')\n",
        "    dynamic_image_path = os.path.join(base_path, f'dynamic_congestion_{i}.png')\n",
        "    accumulated_image_path = os.path.join(base_path, f'accumulated_congestion_{i}.png')\n",
        "\n",
        "    # Excel ê²°ê³¼ ì €ì¥\n",
        "    save_results_to_excel(\n",
        "        pedestrians=pedestrians,\n",
        "        filename=excel_filename,\n",
        "        accumulated_congestion_grid=accumulated_congestion_grid,\n",
        "        max_congestion_per_step=max_congestion_per_step\n",
        "    )\n",
        "\n",
        "    # ê·¸ë¦¼ ì €ì¥\n",
        "    visualize_accumulated_congestion(\n",
        "    area_gdf, buildings_gdf, evacuation_target_area_gdf,\n",
        "    accumulated_congestion_grid, x_coords, y_coords,\n",
        "    save_path=accumulated_image_path\n",
        "    )\n",
        "\n",
        "    visualize_accumulated_congestion(\n",
        "        area_gdf, buildings_gdf, evacuation_target_area_gdf,\n",
        "        dynamic_congestion_grid, x_coords, y_coords,\n",
        "        save_path=dynamic_image_path\n",
        "    )\n",
        "    print(f\"Simulation {i} completed and results saved to:\\n- {excel_filename}\\n- {dynamic_image_path}\\n- {accumulated_image_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1943c60d-ed85-4db7-9dd3-bfc08be1ea2f",
      "metadata": {
        "id": "1943c60d-ed85-4db7-9dd3-bfc08be1ea2f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4fd73e82-3ce9-46ca-8cb3-3257b0e8dcab",
      "metadata": {
        "id": "4fd73e82-3ce9-46ca-8cb3-3257b0e8dcab"
      },
      "source": [
        "# í˜¼ì¡ë„ ì¬í™•ì¸ 1m^2ë‹¹ ê¸°ì¤€ (ê²©ì 1ë‚˜ì”©ë§Œ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0282ae1-b5e8-4904-96cc-fd9bbda19b09",
      "metadata": {
        "id": "c0282ae1-b5e8-4904-96cc-fd9bbda19b09"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ë°ì´í„°\"\n",
        "\n",
        "# ê²©ì ë°ì´í„° ë¡œë“œ\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° íŒŒì¼\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# ì‹œê°„ëŒ€ë³„ ê²©ì í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜\n",
        "def calculate_congestion_per_time_step(congestion_paths):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        positions_at_t = []\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                position = path[t]\n",
        "                if isinstance(position, tuple):  # ë°ì´í„°ê°€ tupleì¸ì§€ í™•ì¸\n",
        "                    positions_at_t.append(position)\n",
        "\n",
        "        position_counts = Counter(positions_at_t)\n",
        "        time_step_congestion.append(position_counts)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# í˜¼ì¡ ê²½ë¡œ ë°ì´í„° ë¡œë“œ ë° í˜¼ì¡ë„ ê³„ì‚°\n",
        "data = pd.read_excel(simulation_file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "time_step_congestion = calculate_congestion_per_time_step(congestion_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82316f16-fbe8-4ffa-bb4a-0b7f58232a73",
      "metadata": {
        "id": "82316f16-fbe8-4ffa-bb4a-0b7f58232a73"
      },
      "outputs": [],
      "source": [
        "max_congestion = max(time_step_congestion[0].items(), key=lambda x: x[1])\n",
        "print(f\"ìµœëŒ€ í˜¼ì¡ë„: {max_congestion[1]}, ìœ„ì¹˜: {max_congestion[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e8a5d5-ca98-437b-a7b6-005dcc428a5b",
      "metadata": {
        "id": "35e8a5d5-ca98-437b-a7b6-005dcc428a5b"
      },
      "source": [
        "# íš¬ìŸ™ë„ ì¬í™•ì¸ 3x3ê¸°ì¤€ìœ¼ë¡œ m^2/ì¸ ë„ì¸¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2cc140-7193-43bc-9e96-4823f21e43c4",
      "metadata": {
        "id": "fc2cc140-7193-43bc-9e96-4823f21e43c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ë°ì´í„°\"\n",
        "\n",
        "# ê²©ì ë°ì´í„° ë¡œë“œ\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore ì´ì›ƒ ê¸°ë°˜ í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜\n",
        "def calculate_congestion_with_moore(congestion_paths, grid_shape):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        congestion_map = np.zeros(grid_shape)\n",
        "\n",
        "        # ê° agentì˜ ìœ„ì¹˜ë¥¼ ì—…ë°ì´íŠ¸\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                x, y = path[t]\n",
        "                if 0 <= x < grid_shape[1] and 0 <= y < grid_shape[0]:\n",
        "                    congestion_map[y, x] += 1\n",
        "\n",
        "        # Moore ì´ì›ƒ ê³„ì‚°\n",
        "        extended_congestion_map = np.zeros_like(congestion_map)\n",
        "        for y in range(grid_shape[0]):\n",
        "            for x in range(grid_shape[1]):\n",
        "                # 3x3 ì˜ì—­ì˜ í•© ê³„ì‚°\n",
        "                for dy in range(-1, 2):\n",
        "                    for dx in range(-1, 2):\n",
        "                        ny, nx = y + dy, x + dx\n",
        "                        if 0 <= nx < grid_shape[1] and 0 <= ny < grid_shape[0]:\n",
        "                            extended_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "\n",
        "        # ë©´ì  ëŒ€ë¹„ í˜¼ì¡ë„ ê³„ì‚° (m^2/ì¸)\n",
        "        cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # í•œ ê²©ìì˜ ë©´ì \n",
        "        extended_congestion_map /= cell_area\n",
        "        time_step_congestion.append(extended_congestion_map)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# í˜¼ì¡ ê²½ë¡œ ë°ì´í„° ë¡œë“œ ë° í˜¼ì¡ë„ ê³„ì‚°\n",
        "data = pd.read_excel(os.path.join(base_path, \"simulation_results_1.xlsx\"), sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "time_step_congestion = calculate_congestion_with_moore(congestion_paths, grid.shape)\n",
        "\n",
        "# íŠ¹ì • time step í˜¼ì¡ë„ í™•ì¸ (ì˜ˆ: ì²« ë²ˆì§¸ time step)\n",
        "time_step = 0\n",
        "print(f\"í˜¼ì¡ë„ ë§µ (time step {time_step + 1}):\")\n",
        "print(time_step_congestion[time_step])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2687f6f5-e7ae-44fa-9fbb-14bbfe56030f",
      "metadata": {
        "id": "2687f6f5-e7ae-44fa-9fbb-14bbfe56030f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ë°ì´í„°\"\n",
        "\n",
        "# ê²©ì ë°ì´í„° ë¡œë“œ\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore ì´ì›ƒ ê¸°ë°˜ í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜\n",
        "def calculate_congestion_with_moore(congestion_paths, grid_shape):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        congestion_map = np.zeros(grid_shape)\n",
        "\n",
        "        # ê° agentì˜ ìœ„ì¹˜ë¥¼ ì—…ë°ì´íŠ¸\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                x, y = path[t]\n",
        "                if 0 <= x < grid_shape[1] and 0 <= y < grid_shape[0]:\n",
        "                    congestion_map[y, x] += 1\n",
        "\n",
        "        # Moore ì´ì›ƒ ê³„ì‚°\n",
        "        extended_congestion_map = np.zeros_like(congestion_map)\n",
        "        for y in range(grid_shape[0]):\n",
        "            for x in range(grid_shape[1]):\n",
        "                # 3x3 ì˜ì—­ì˜ í•© ê³„ì‚° (ê°€ëŠ¥í•œ ì˜ì—­ë§Œ í¬í•¨)\n",
        "                for dy in range(-1, 2):\n",
        "                    for dx in range(-1, 2):\n",
        "                        ny, nx = y + dy, x + dx\n",
        "                        if 0 <= nx < grid_shape[1] and 0 <= ny < grid_shape[0]:\n",
        "                            extended_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "\n",
        "        # ë©´ì  ëŒ€ë¹„ í˜¼ì¡ë„ ê³„ì‚° (m^2/ì¸)\n",
        "        cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # í•œ ê²©ìì˜ ë©´ì \n",
        "        extended_congestion_map /= cell_area\n",
        "        time_step_congestion.append(extended_congestion_map)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# ì‹œë®¬ë ˆì´ì…˜ íŒŒì¼ ì²˜ë¦¬\n",
        "simulation_files = [os.path.join(base_path, f\"simulation_results_{i}.xlsx\") for i in range(1, 10)]\n",
        "all_simulations = []\n",
        "\n",
        "for i, file in enumerate(simulation_files, start=1):\n",
        "    try:\n",
        "        # ë°ì´í„° ë¡œë“œ\n",
        "        data = pd.read_excel(file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "        congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "        time_step_congestion = calculate_congestion_with_moore(congestion_paths, grid.shape)\n",
        "\n",
        "        # ê° time stepì˜ í‰ê·  í˜¼ì¡ë„ ê³„ì‚°\n",
        "        avg_congestions = [\n",
        "            np.mean(step) if step.size > 0 else 0  # í‰ê·  í˜¼ì¡ë„ ê³„ì‚°\n",
        "            for step in time_step_congestion\n",
        "        ]\n",
        "\n",
        "        # ê¸¸ì´ ê³ ì •: 1000ê°œì˜ time stepìœ¼ë¡œ ë§ì¶”ê¸°\n",
        "        if len(avg_congestions) < 1000:\n",
        "            avg_congestions.extend([avg_congestions[-1]] * (1000 - len(avg_congestions)))\n",
        "        else:\n",
        "            avg_congestions = avg_congestions[:1000]\n",
        "\n",
        "        time_steps = list(range(1, 1001))\n",
        "\n",
        "        # DataFrame ìƒì„±\n",
        "        df = pd.DataFrame({\n",
        "            \"Time Step\": time_steps,\n",
        "            f\"ëŒ€í”¼ì†Œ ì¸ì§€ ë¹„ìœ¨ {100 - i * 10}%\": avg_congestions\n",
        "        })\n",
        "\n",
        "        if len(all_simulations) == 0:\n",
        "            all_simulations = df\n",
        "        else:\n",
        "            all_simulations = pd.merge(all_simulations, df, on=\"Time Step\", how=\"outer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# í•˜ë‚˜ì˜ ì‹œíŠ¸ë¡œ ì €ì¥\n",
        "output_file = os.path.join(base_path, \"í˜¼ì¡ë„ê³„ì‚°_í™•ì¥ê¸°ì¤€.xlsx\")\n",
        "all_simulations.to_excel(output_file, index=False)\n",
        "print(f\"ëª¨ë“  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71bc7f84-f6ab-492c-a9ed-373c082c0462",
      "metadata": {
        "id": "71bc7f84-f6ab-492c-a9ed-373c082c0462"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ë°ì´í„°\"\n",
        "\n",
        "# ê²©ì ë°ì´í„° ë¡œë“œ\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore ì´ì›ƒ í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜ (í˜¼ì¡ë„ê°€ ì¡´ì¬í•˜ëŠ” ì…€ë§Œ ê³„ì‚°)\n",
        "def calculate_moore_congestion(grid, agent_positions):\n",
        "    congestion_map = np.zeros_like(grid, dtype=float)  # float íƒ€ì…ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "\n",
        "    # agent ìœ„ì¹˜ë¥¼ gridì— ë§µí•‘\n",
        "    for x, y in agent_positions:\n",
        "        if 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:\n",
        "            congestion_map[y, x] += 1\n",
        "\n",
        "    # Moore ì´ì›ƒ ê³„ì‚° (í˜¼ì¡ë„ê°€ ì¡´ì¬í•˜ëŠ” ì…€ë§Œ)\n",
        "    moore_congestion_map = np.zeros_like(congestion_map, dtype=float)  # float íƒ€ì…ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "    active_cells = np.argwhere(congestion_map > 0)\n",
        "\n",
        "    for y, x in active_cells:\n",
        "        cell_count = 0  # í¬í•¨ëœ ì…€ ê°œìˆ˜\n",
        "        for dy in range(-1, 2):\n",
        "            for dx in range(-1, 2):\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < grid.shape[0] and 0 <= nx < grid.shape[1]:\n",
        "                    moore_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "                    cell_count += 1  # í¬í•¨ëœ ì…€ ìˆ˜ ì¦ê°€\n",
        "        if cell_count > 0:\n",
        "            moore_congestion_map[y, x] /= cell_count  # ì…€ ê°œìˆ˜ë¡œ ë‚˜ëˆ”\n",
        "\n",
        "    return moore_congestion_map\n",
        "\n",
        "# ì‹œë‚˜ë¦¬ì˜¤ 1 ë°ì´í„° ì²˜ë¦¬\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "try:\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "\n",
        "    # ì‹œê°„ëŒ€ë³„ í‰ê·  í˜¼ì¡ë„ ì €ì¥\n",
        "    avg_congestions = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        # agent ìœ„ì¹˜ ë³µì›\n",
        "        agent_positions = [path[t] for path in congestion_paths if t < len(path)]\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions)\n",
        "        avg_congestions.append(np.mean(moore_congestion_map[moore_congestion_map > 0]))\n",
        "\n",
        "    # ê¸¸ì´ ê³ ì •: 1000ê°œì˜ time stepìœ¼ë¡œ ë§ì¶”ê¸°\n",
        "    if len(avg_congestions) < 1000:\n",
        "        avg_congestions.extend([avg_congestions[-1]] * (1000 - len(avg_congestions)))\n",
        "    else:\n",
        "        avg_congestions = avg_congestions[:1000]\n",
        "\n",
        "    time_steps = list(range(1, 1001))\n",
        "\n",
        "    # DataFrame ìƒì„±\n",
        "    df = pd.DataFrame({\n",
        "        \"Time Step\": time_steps,\n",
        "        \"ëŒ€í”¼ì†Œ ì¸ì§€ ë¹„ìœ¨ 90%\": avg_congestions\n",
        "    })\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    output_file = os.path.join(base_path, \"í˜¼ì¡ë„ê³„ì‚°_ìµœì í™”_ì‹œë‚˜ë¦¬ì˜¤1.xlsx\")\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"ì‹œë‚˜ë¦¬ì˜¤ 1 ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13dd00a6-2620-4f16-995e-6f6fdc645788",
      "metadata": {
        "id": "13dd00a6-2620-4f16-995e-6f6fdc645788"
      },
      "outputs": [],
      "source": [
        "moore_congestion_map\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31896c98-9480-42ac-986f-57629199ed9e",
      "metadata": {
        "id": "31896c98-9480-42ac-986f-57629199ed9e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ë°ì´í„°\"\n",
        "\n",
        "# ê²©ì ë°ì´í„° ë¡œë“œ\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore ì´ì›ƒ í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜ (í˜¼ì¡ë„ê°€ ì¡´ì¬í•˜ëŠ” ì…€ë§Œ ê³„ì‚°)\n",
        "def calculate_moore_congestion(grid, agent_positions, cell_area):\n",
        "    congestion_map = np.zeros_like(grid, dtype=float)  # float íƒ€ì…ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "\n",
        "    # agent ìœ„ì¹˜ë¥¼ gridì— ë§µí•‘\n",
        "    for x, y in agent_positions:\n",
        "        if 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:\n",
        "            congestion_map[y, x] += 1\n",
        "\n",
        "    # Moore ì´ì›ƒ ê³„ì‚° (í˜¼ì¡ë„ê°€ ì¡´ì¬í•˜ëŠ” ì…€ë§Œ)\n",
        "    moore_congestion_map = np.zeros_like(congestion_map, dtype=float)  # float íƒ€ì…ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "    active_cells = np.argwhere(congestion_map > 0)\n",
        "\n",
        "    for y, x in active_cells:\n",
        "        cell_count = 0  # í¬í•¨ëœ ì…€ ê°œìˆ˜\n",
        "        for dy in range(-1, 2):\n",
        "            for dx in range(-1, 2):\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < grid.shape[0] and 0 <= nx < grid.shape[1]:\n",
        "                    moore_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "                    cell_count += 1  # í¬í•¨ëœ ì…€ ìˆ˜ ì¦ê°€\n",
        "        if cell_count > 0:\n",
        "            moore_congestion_map[y, x] /= (cell_area * cell_count)  # \\( \\text{m}^2/\\text{ì¸} \\) ê³„ì‚°\n",
        "\n",
        "    return moore_congestion_map\n",
        "\n",
        "# ì‹œë‚˜ë¦¬ì˜¤ 1 ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "try:\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "\n",
        "    # ì…€ ë©´ì  ê³„ì‚°\n",
        "    cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # \\( \\text{m}^2 \\)\n",
        "\n",
        "    # ì‹œê°í™”í•  time step ëª©ë¡\n",
        "    time_steps_to_visualize = [0, 499, 999]  # 0ì´ˆ, 500ì´ˆ, 1000ì´ˆ\n",
        "    for time_step in time_steps_to_visualize:\n",
        "        # agent ìœ„ì¹˜ ë³µì›\n",
        "        agent_positions = [path[time_step] for path in congestion_paths if time_step < len(path)]\n",
        "\n",
        "        # Moore Congestion Map ê³„ì‚°\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions, cell_area)\n",
        "\n",
        "        # ì‹œê°í™”\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(moore_congestion_map, extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], origin='lower', cmap='hot')\n",
        "        plt.colorbar(label=\"Congestion (agents/mÂ²)\")\n",
        "        plt.title(f\"Moore Congestion Map at Time-step {time_step + 1}\")\n",
        "        plt.xlabel(\"X Coordinate\")\n",
        "        plt.ylabel(\"Y Coordinate\")\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa6d33d-3f9b-4c94-8405-e6a406112cf6",
      "metadata": {
        "id": "6fa6d33d-3f9b-4c94-8405-e6a406112cf6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "grid_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ë°ì´í„°\"\n",
        "\n",
        "# ê²©ì ë°ì´í„° ë¡œë“œ\n",
        "grid = np.load(os.path.join(grid_path, \"grid.npy\"))\n",
        "x_coords = np.loadtxt(os.path.join(grid_path, \"x_coords.csv\"), delimiter=\",\")\n",
        "y_coords = np.loadtxt(os.path.join(grid_path, \"y_coords.csv\"), delimiter=\",\")\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# Moore ì´ì›ƒ í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜ (í˜¼ì¡ë„ê°€ ì¡´ì¬í•˜ëŠ” ì…€ë§Œ ê³„ì‚°)\n",
        "def calculate_moore_congestion(grid, agent_positions, cell_area):\n",
        "    congestion_map = np.zeros_like(grid, dtype=float)  # float íƒ€ì…ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "\n",
        "    # agent ìœ„ì¹˜ë¥¼ gridì— ë§µí•‘\n",
        "    for x, y in agent_positions:\n",
        "        if 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:\n",
        "            congestion_map[y, x] += 1\n",
        "\n",
        "    # Moore ì´ì›ƒ ê³„ì‚° (í˜¼ì¡ë„ê°€ ì¡´ì¬í•˜ëŠ” ì…€ë§Œ)\n",
        "    moore_congestion_map = np.zeros_like(congestion_map, dtype=float)  # float íƒ€ì…ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "    active_cells = np.argwhere(congestion_map > 0)\n",
        "\n",
        "    for y, x in active_cells:\n",
        "        cell_count = 0  # í¬í•¨ëœ ì…€ ê°œìˆ˜\n",
        "        for dy in range(-1, 2):\n",
        "            for dx in range(-1, 2):\n",
        "                ny, nx = y + dy, x + dx\n",
        "                if 0 <= ny < grid.shape[0] and 0 <= nx < grid.shape[1]:\n",
        "                    moore_congestion_map[y, x] += congestion_map[ny, nx]\n",
        "                    cell_count += 1  # í¬í•¨ëœ ì…€ ìˆ˜ ì¦ê°€\n",
        "        if cell_count > 0:\n",
        "             moore_congestion_map[y, x]/= (cell_area * cell_count)  # \\( \\text{m}^2/\\text{ì¸} \\) ë³€í™˜\n",
        "\n",
        "    return moore_congestion_map\n",
        "\n",
        "# ì‹œë‚˜ë¦¬ì˜¤ 1 ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_9.xlsx\")\n",
        "\n",
        "try:\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "\n",
        "    # ì…€ ë©´ì  ê³„ì‚°\n",
        "    cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # \\( \\text{m}^2 \\)\n",
        "\n",
        "    # ì‹œê°í™”í•  time step ëª©ë¡\n",
        "    time_steps_to_visualize = [0, 499, 999]  # 0ì´ˆ, 500ì´ˆ, 1000ì´ˆ\n",
        "    for time_step in time_steps_to_visualize:\n",
        "        # agent ìœ„ì¹˜ ë³µì›\n",
        "        agent_positions = [path[time_step] for path in congestion_paths if time_step < len(path)]\n",
        "\n",
        "        # Moore Congestion Map ê³„ì‚°\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions, cell_area)\n",
        "\n",
        "        # ì‹œê°í™”\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(moore_congestion_map, extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], origin='lower', cmap='hot')\n",
        "        plt.colorbar(label=\"Congestion (mÂ²/agent)\")\n",
        "        plt.title(f\"Moore Congestion Map at Time-step {time_step + 1}\")\n",
        "        plt.xlabel(\"X Coordinate\")\n",
        "        plt.ylabel(\"Y Coordinate\")\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcaeeec6-692b-424e-82ee-ddb56e8fe497",
      "metadata": {
        "id": "dcaeeec6-692b-424e-82ee-ddb56e8fe497"
      },
      "outputs": [],
      "source": [
        "# ì‹œë‚˜ë¦¬ì˜¤ 1 ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”\n",
        "simulation_file = os.path.join(base_path, \"simulation_results_1.xlsx\")\n",
        "\n",
        "try:\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    data = pd.read_excel(simulation_file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "    congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "\n",
        "    # ì…€ ë©´ì  ê³„ì‚°\n",
        "    cell_area = (x_coords[1] - x_coords[0]) * (y_coords[1] - y_coords[0])  # \\( \\text{m}^2 \\)\n",
        "\n",
        "    # ì‹œê°í™”í•  time step ëª©ë¡\n",
        "    time_steps_to_visualize = [0, 499, 999]  # 0ì´ˆ, 500ì´ˆ, 1000ì´ˆ\n",
        "    for time_step in time_steps_to_visualize:\n",
        "        # agent ìœ„ì¹˜ ë³µì›\n",
        "        agent_positions = [path[time_step] for path in congestion_paths if time_step < len(path)]\n",
        "\n",
        "        # Moore Congestion Map ê³„ì‚°\n",
        "        moore_congestion_map = calculate_moore_congestion(grid, agent_positions, cell_area)\n",
        "\n",
        "        # ì‹œê°í™”\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(moore_congestion_map, extent=[x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max()], origin='lower', cmap='hot')\n",
        "        plt.colorbar(label=\"Congestion (mÂ²/agent)\")\n",
        "        plt.title(f\"Moore Congestion Map at Time-step {time_step + 1}\")\n",
        "        plt.xlabel(\"X Coordinate\")\n",
        "        plt.ylabel(\"Y Coordinate\")\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing {simulation_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993c8e50-3ac5-450c-8dbd-548fb3e7b744",
      "metadata": {
        "id": "993c8e50-3ac5-450c-8dbd-548fb3e7b744"
      },
      "source": [
        "## ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5615f432-423f-455a-8dc7-c8b250273363",
      "metadata": {
        "id": "5615f432-423f-455a-8dc7-c8b250273363"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "\n",
        "# ê° time stepì—ì„œ ìµœëŒ€ í˜¼ì¡ë„ë¥¼ ê³„ì‚°\n",
        "max_congestions = [\n",
        "    max(step.values()) if step else 0  # stepì´ ë¹„ì–´ìˆì„ ê²½ìš° 0 ì²˜ë¦¬\n",
        "    for step in time_step_congestion\n",
        "]\n",
        "\n",
        "# ì‹œê°„ ì¶• ìƒì„± (1ì´ˆë¶€í„° 1000ì´ˆê¹Œì§€ë§Œ)\n",
        "time_steps = range(1, 1001)\n",
        "\n",
        "# 1000ì´ˆê¹Œì§€ë§Œ ë°ì´í„° ìŠ¬ë¼ì´ì‹±\n",
        "max_congestions = max_congestions[:1000]\n",
        "\n",
        "# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(time_steps, max_congestions, color='blue', label=\"ìµœëŒ€ í˜¼ì¡ë„\", alpha=0.8)\n",
        "plt.title(\"ê° ì‹œê°„ë³„ ìµœëŒ€ í˜¼ì¡ë„\", fontsize=14)\n",
        "plt.xlabel(\"ì‹œê°„(ì´ˆ)\", fontsize=12)\n",
        "plt.ylabel(\"ìµœëŒ€ í˜¼ì¡ë„\", fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c3f517-34ce-4983-bc6a-5aaea000dea0",
      "metadata": {
        "id": "81c3f517-34ce-4983-bc6a-5aaea000dea0"
      },
      "source": [
        "ì‹œë‚˜ë¦¬ì˜¤ 8\n",
        "\n",
        "\n",
        "![download.png](attachment:631f963d-ebe5-4442-9579-febb2cd2857d.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489bc266-3585-4675-9909-26ce04a99a3b",
      "metadata": {
        "id": "489bc266-3585-4675-9909-26ce04a99a3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "\n",
        "# ë°ì´í„° ë³µêµ¬: str -> list ë³€í™˜ í•¨ìˆ˜\n",
        "def parse_congestion_path(congestion_path_str):\n",
        "    try:\n",
        "        return ast.literal_eval(congestion_path_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing congestion path: {e}\")\n",
        "        return []\n",
        "\n",
        "# ì‹œê°„ëŒ€ë³„ ê²©ì í˜¼ì¡ë„ ê³„ì‚° í•¨ìˆ˜\n",
        "def calculate_congestion_per_time_step(congestion_paths):\n",
        "    time_step_congestion = []\n",
        "    max_time_steps = max(len(path) for path in congestion_paths)\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        positions_at_t = []\n",
        "        for path in congestion_paths:\n",
        "            if t < len(path):\n",
        "                position = path[t]\n",
        "                if isinstance(position, tuple):  # ë°ì´í„°ê°€ tupleì¸ì§€ í™•ì¸\n",
        "                    positions_at_t.append(position)\n",
        "\n",
        "        position_counts = Counter(positions_at_t)\n",
        "        time_step_congestion.append(position_counts)\n",
        "\n",
        "    return time_step_congestion\n",
        "\n",
        "# ì‹œë®¬ë ˆì´ì…˜ íŒŒì¼ ì²˜ë¦¬\n",
        "simulation_files = [os.path.join(base_path, f\"simulation_results_{i}.xlsx\") for i in range(1, 10)]\n",
        "all_simulations = []\n",
        "\n",
        "for i, file in enumerate(simulation_files, start=1):\n",
        "    try:\n",
        "        # ë°ì´í„° ë¡œë“œ\n",
        "        data = pd.read_excel(file, sheet_name=\"ë³´í–‰ì ë°ì´í„°\", usecols=[\"í˜¼ì¡ ê²½ë¡œ\"])\n",
        "        congestion_paths = [parse_congestion_path(path) for path in data[\"í˜¼ì¡ ê²½ë¡œ\"].tolist()]\n",
        "        time_step_congestion = calculate_congestion_per_time_step(congestion_paths)\n",
        "\n",
        "        # ê° time stepì˜ ìµœëŒ€ í˜¼ì¡ë„ ê³„ì‚°\n",
        "        max_congestions = [\n",
        "            max(step.values()) if step else 0  # ë¹„ì–´ìˆìœ¼ë©´ 0 ì²˜ë¦¬\n",
        "            for step in time_step_congestion\n",
        "        ]\n",
        "\n",
        "        # ê¸¸ì´ ê³ ì •: 1000ê°œì˜ time stepìœ¼ë¡œ ë§ì¶”ê¸°\n",
        "        if len(max_congestions) < 1000:\n",
        "            max_congestions.extend([max_congestions[-1]] * (1000 - len(max_congestions)))\n",
        "        else:\n",
        "            max_congestions = max_congestions[:1000]\n",
        "\n",
        "        time_steps = list(range(1, 1001))\n",
        "\n",
        "        # DataFrame ìƒì„±\n",
        "        df = pd.DataFrame({\n",
        "            \"Time Step\": time_steps,\n",
        "            f\"ëŒ€í”¼ì†Œ ì¸ì§€ ë¹„ìœ¨ {100 - i * 10}%\": max_congestions\n",
        "        })\n",
        "\n",
        "        if len(all_simulations) == 0:\n",
        "            all_simulations = df\n",
        "        else:\n",
        "            all_simulations = pd.merge(all_simulations, df, on=\"Time Step\", how=\"outer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# í•˜ë‚˜ì˜ ì‹œíŠ¸ë¡œ ì €ì¥\n",
        "output_file = os.path.join(base_path, \"simulation_results_combined.xlsx\")\n",
        "all_simulations.to_excel(output_file, index=False)\n",
        "print(f\"ëª¨ë“  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef69553-3578-4ae0-9757-1c957ac2eadb",
      "metadata": {
        "id": "aef69553-3578-4ae0-9757-1c957ac2eadb"
      },
      "outputs": [],
      "source": [
        "simulation_congestion_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119bda48-0770-4ab3-bbec-00a3c91dc3cb",
      "metadata": {
        "id": "119bda48-0770-4ab3-bbec-00a3c91dc3cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aae4ada-cca9-4b99-82f2-a84a4ee8047c",
      "metadata": {
        "id": "3aae4ada-cca9-4b99-82f2-a84a4ee8047c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57cef978-1941-4f44-bdfc-0af5a6adaabb",
      "metadata": {
        "id": "57cef978-1941-4f44-bdfc-0af5a6adaabb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4a43aa1b-5975-4ee4-b2ff-ee83d8aea471",
      "metadata": {
        "id": "4a43aa1b-5975-4ee4-b2ff-ee83d8aea471"
      },
      "source": [
        "# ê·¸ë˜í”„ ë§Œë“¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d48315-9145-4669-a4e7-029194e13fb8",
      "metadata": {
        "id": "b7d48315-9145-4669-a4e7-029194e13fb8"
      },
      "outputs": [],
      "source": [
        "# ê·¸ë˜í”„: ëŒ€í”¼ì†Œ ì¸ì§€ì— ë”°ë¥¸ ëˆ„ì  í˜¼ì¡ë„ ë° ë™ì  í˜¼ì¡ë„ ë³€í™”\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "base_path = r\"C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\"\n",
        "simulation_files = [os.path.join(base_path, f\"simulation_results_{i}.xlsx\") for i in range(1, 10)]\n",
        "\n",
        "# Exploratory ë¹„ìœ¨\n",
        "exploratory_ratios = [0.1 * i for i in range(1, 10)]\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ ë³€ìˆ˜\n",
        "max_accumulated_congestions = []\n",
        "mean_accumulated_congestions = []\n",
        "max_dynamic_congestions = []\n",
        "mean_dynamic_congestions = []\n",
        "\n",
        "# ë°ì´í„° ì¶”ì¶œ\n",
        "for file in simulation_files:\n",
        "    data = pd.ExcelFile(file)\n",
        "\n",
        "    # ëˆ„ì  í˜¼ì¡ë„ í†µê³„\n",
        "    accumulated_stats = pd.read_excel(data, sheet_name=\"ëˆ„ì  í˜¼ì¡ë„ í†µê³„\", header=1)\n",
        "    max_accumulated_congestions.append(accumulated_stats.loc[accumulated_stats.iloc[:, 0] == 'max', accumulated_stats.columns[1]].values[0])\n",
        "    mean_accumulated_congestions.append(accumulated_stats.loc[accumulated_stats.iloc[:, 0] == 'mean', accumulated_stats.columns[1]].values[0])\n",
        "\n",
        "    # ë™ì  í˜¼ì¡ë„ í†µê³„\n",
        "    dynamic_stats = pd.read_excel(data, sheet_name=\"ë™ì  í˜¼ì¡ë„ í†µê³„\", header=1)\n",
        "    max_dynamic_congestions.append(dynamic_stats.loc[dynamic_stats.iloc[:, 0] == 'max', dynamic_stats.columns[1]].values[0])\n",
        "    mean_dynamic_congestions.append(dynamic_stats.loc[dynamic_stats.iloc[:, 0] == 'mean', dynamic_stats.columns[1]].values[0])\n",
        "\n",
        "    # ì „ì²´ í†µê³„ì—ì„œ í‰ê·  í˜¼ì¡ë„ ì¶”ì¶œ\n",
        "    overall_stats = pd.read_excel(data, sheet_name=\"ì „ì²´ í†µê³„\")\n",
        "    avg_congestion = overall_stats.iloc[0, 2]  # í‰ê·  í˜¼ì¡ë„ê°€ 3ë²ˆì§¸ ì—´ì— ìˆë‹¤ê³  ê°€ì •\n",
        "    avg_congestions.append(avg_congestion)\n",
        "\n",
        "# ê·¸ë˜í”„ ìƒì„±: ëˆ„ì  í˜¼ì¡ë„\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, max_accumulated_congestions, marker='o', label='ëˆ„ì  í˜¼ì¡ë„ ìµœëŒ“ê°’', color='red')\n",
        "plt.plot(exploratory_ratios, mean_accumulated_congestions, marker='s', label='ëˆ„ì  í˜¼ì¡ë„ í‰ê· ê°’', color='blue')\n",
        "plt.title(\"ëŒ€í”¼ì†Œ ì¸ì§€ì— ë”°ë¥¸ ëˆ„ì  í˜¼ì¡ë„ ë³€í™”\", fontsize=14)\n",
        "plt.xlabel(\"ëŒ€í”¼ì†Œ ì¸ì§€ ëª»í•œ ëŒ€í”¼ì ë¹„ìœ¨\", fontsize=12)\n",
        "plt.ylabel(\"ëˆ„ì  í˜¼ì¡ë„\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(base_path, \"accumulated_congestion_vs_exploratory.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ê·¸ë˜í”„ ìƒì„±: ë™ì  í˜¼ì¡ë„\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, max_dynamic_congestions, marker='o', label='ë™ì  í˜¼ì¡ë„ ìµœëŒ“ê°’', color='green')\n",
        "plt.plot(exploratory_ratios, mean_dynamic_congestions, marker='s', label='ë™ì  í˜¼ì¡ë„ í‰ê· ê°’', color='orange')\n",
        "plt.title(\"ëŒ€í”¼ì†Œ ì¸ì§€ì— ë”°ë¥¸ ë™ì  í˜¼ì¡ë„ ë³€í™”\", fontsize=14)\n",
        "plt.xlabel(\"ëŒ€í”¼ì†Œ ì¸ì§€ ëª»í•œ ëŒ€í”¼ì ë¹„ìœ¨\", fontsize=12)\n",
        "plt.ylabel(\"ë™ì  í˜¼ì¡ë„\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(base_path, \"dynamic_congestion_vs_exploratory.png\"), dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cda82e-9ca2-4635-bc9c-381db57226cb",
      "metadata": {
        "id": "c9cda82e-9ca2-4635-bc9c-381db57226cb"
      },
      "outputs": [],
      "source": [
        "# ê·¸ë˜í”„ ìƒì„±: ëˆ„ì  í˜¼ì¡ë„\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, mean_accumulated_congestions, marker='s', label='ëˆ„ì  í˜¼ì¡ë„ í‰ê· ê°’', color='blue')\n",
        "plt.title(\"ëŒ€í”¼ì†Œ ì¸ì§€ì— ë”°ë¥¸ ëˆ„ì  í˜¼ì¡ë„ ë³€í™”\", fontsize=14)\n",
        "plt.xlabel(\"ëŒ€í”¼ì†Œ ì¸ì§€ ëª»í•œ ëŒ€í”¼ì ë¹„ìœ¨\", fontsize=12)\n",
        "plt.ylabel(\"ëˆ„ì  í˜¼ì¡ë„\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab379f75-a058-43e6-bd38-ef9319d66948",
      "metadata": {
        "id": "ab379f75-a058-43e6-bd38-ef9319d66948"
      },
      "outputs": [],
      "source": [
        "# ê·¸ë˜í”„ ë° ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€\n",
        "avg_congestions = []  # ì „ì²´ í†µê³„ ì‹œíŠ¸ì˜ í‰ê·  í˜¼ì¡ë„ ì €ì¥\n",
        "\n",
        "# ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œì— ì´ì–´ì§)\n",
        "for file in simulation_files:\n",
        "    data = pd.ExcelFile(file)\n",
        "\n",
        "    # ì „ì²´ í†µê³„ì—ì„œ í‰ê·  í˜¼ì¡ë„ ì¶”ì¶œ\n",
        "    overall_stats = pd.read_excel(data, sheet_name=\"ì „ì²´ í†µê³„\")\n",
        "    avg_congestion = overall_stats.loc[0, 'í‰ê·  í˜¼ì¡ë„']\n",
        "    avg_congestions.append(avg_congestion)\n",
        "\n",
        "# ê·¸ë˜í”„ ìƒì„±: í‰ê·  í˜¼ì¡ë„\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, avg_congestions, marker='o', label='í‰ê·  í˜¼ì¡ë„', color='purple')\n",
        "plt.title(\"ëŒ€í”¼ì†Œ ì¸ì§€ì— ë”°ë¥¸ í‰ê·  í˜¼ì¡ë„ ë³€í™”\", fontsize=14)\n",
        "plt.xlabel(\"ëŒ€í”¼ì†Œ ì¸ì§€ ëª»í•œ Exploratory ë¹„ìœ¨\", fontsize=12)\n",
        "plt.ylabel(\"í‰ê·  í˜¼ì¡ë„\", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(base_path, \"average_congestion_vs_exploratory.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ë°ì´í„° ì €ì¥: í˜¼ì¡ë„ ë° í‰ê·  í˜¼ì¡ë„ ì—‘ì…€ë¡œ ì €ì¥\n",
        "output_data = pd.DataFrame({\n",
        "    'Exploratory ë¹„ìœ¨': exploratory_ratios,\n",
        "    'ëˆ„ì  í˜¼ì¡ë„ ìµœëŒ“ê°’': max_accumulated_congestions,\n",
        "    'ëˆ„ì  í˜¼ì¡ë„ í‰ê· ê°’': mean_accumulated_congestions,\n",
        "    'ë™ì  í˜¼ì¡ë„ ìµœëŒ“ê°’': max_dynamic_congestions,\n",
        "    'ë™ì  í˜¼ì¡ë„ í‰ê· ê°’': mean_dynamic_congestions,\n",
        "    'í‰ê·  í˜¼ì¡ë„': avg_congestions\n",
        "})\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "output_file = os.path.join(base_path, \"congestion_analysis_results.xlsx\")\n",
        "\n",
        "# ì—‘ì…€ë¡œ ì €ì¥\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    output_data.to_excel(writer, index=False, sheet_name='í˜¼ì¡ë„ ë¶„ì„ ê²°ê³¼')\n",
        "\n",
        "print(f\"ê²°ê³¼ê°€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a0b1ed-0a59-4ea9-95fd-afd59e885886",
      "metadata": {
        "id": "d4a0b1ed-0a59-4ea9-95fd-afd59e885886"
      },
      "outputs": [],
      "source": [
        "overall_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c6e5b9-6709-4806-9939-e3d5e3b78e49",
      "metadata": {
        "id": "91c6e5b9-6709-4806-9939-e3d5e3b78e49"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„° ì¶”ì¶œ\n",
        "success_rates=[]\n",
        "for file in simulation_files:\n",
        "    data = pd.ExcelFile(file)\n",
        "# ëŒ€í”¼ ì„±ê³µë¥  ì¶”ì¶œ\n",
        "    overall_stats = pd.read_excel(data, sheet_name=\"ì „ì²´ í†µê³„\")\n",
        "    success_rate = overall_stats.loc[0, 'ëŒ€í”¼ ì„±ê³µë¥ (%)']\n",
        "    success_rates.append(success_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2506d25b-1e19-43ba-abd2-9c129f1088e5",
      "metadata": {
        "id": "2506d25b-1e19-43ba-abd2-9c129f1088e5"
      },
      "outputs": [],
      "source": [
        "# ê·¸ë˜í”„ 2: ëŒ€í”¼ì†Œ ì¸ì§€ ë¹„ìœ¨ì— ë”°ë¥¸ ëŒ€í”¼ ì„±ê³µë¥ \n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(exploratory_ratios, success_rates, marker='o', color='green', label='ëŒ€í”¼ ì„±ê³µë¥ ')\n",
        "plt.title(\"ëŒ€í”¼ì†Œ ì¸ì§€ ë¹„ìœ¨ì— ë”°ë¥¸ ëŒ€í”¼ ì„±ê³µë¥ \", fontsize=14)\n",
        "plt.xlabel(\"ëŒ€í”¼ì†Œ ì¸ì§€ ëª»í•œ Exploratory ë¹„ìœ¨\", fontsize=12)\n",
        "plt.ylabel(\"ëŒ€í”¼ ì„±ê³µë¥ \", fontsize=12)\n",
        "plt.xticks(exploratory_ratios)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48b5a61-5ea6-4ec1-8810-2fb741f0c7ac",
      "metadata": {
        "id": "d48b5a61-5ea6-4ec1-8810-2fb741f0c7ac"
      },
      "outputs": [],
      "source": [
        "success_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23215ff-1272-4c12-b9dc-ea936d0831b4",
      "metadata": {
        "id": "b23215ff-1272-4c12-b9dc-ea936d0831b4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b6039f-49a7-4d9a-9df3-7251609cf8ee",
      "metadata": {
        "id": "71b6039f-49a7-4d9a-9df3-7251609cf8ee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c2e725-0da2-4a71-b4b8-299ce85fb8d5",
      "metadata": {
        "id": "a1c2e725-0da2-4a71-b4b8-299ce85fb8d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce99f2b-78e0-4fd6-8736-a0c653eaf9ad",
      "metadata": {
        "id": "7ce99f2b-78e0-4fd6-8736-a0c653eaf9ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd5ed25-8d42-4d68-9c79-15c0da196f50",
      "metadata": {
        "id": "4bd5ed25-8d42-4d68-9c79-15c0da196f50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816ba33c-7b83-4b46-8fe8-47bc910de2e2",
      "metadata": {
        "id": "816ba33c-7b83-4b46-8fe8-47bc910de2e2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbb8e8c-cbac-48f4-b161-2c8eadc5e365",
      "metadata": {
        "id": "afbb8e8c-cbac-48f4-b161-2c8eadc5e365"
      },
      "outputs": [],
      "source": [
        "1íšŒ í•˜ë˜ë–„ (ë°˜ë³µ ì•ˆí• ë–„)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29416768-53fb-4ad8-8a50-c7d7e9abf989",
      "metadata": {
        "id": "29416768-53fb-4ad8-8a50-c7d7e9abf989"
      },
      "outputs": [],
      "source": [
        "# ë³´í–‰ì ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "total_agents = 1000\n",
        "special_area_ratio = 0.3\n",
        "agent_type_ratios = {'ë…¸ì¸/ì–´ë¦°ì´': 0.29, 'ì¤‘ì¥ë…„': 0.34, 'ì²­ì†Œë…„/ì²­ë…„': 0.37, 'ì¥ì• ì¸': 0.07}\n",
        "agent_types = {'ë…¸ì¸/ì–´ë¦°ì´': 1.0, 'ì¤‘ì¥ë…„': 1.3, 'ì²­ì†Œë…„/ì²­ë…„': 1.9, 'ì¥ì• ì¸': 0.71}\n",
        "behaviors = {'knows_specific': 0.3, 'knows_all': 0.4, 'exploratory': 0.3}\n",
        "# ë³´í–‰ì ìƒì„±\n",
        "pedestrians = generate_pedestrians(\n",
        "    total_agents, agent_type_ratios, agent_types, behaviors,\n",
        "    special_area_cells, general_area_cells, evacuation_target_area_cells,\n",
        "    x_coords, y_coords, special_area_ratio\n",
        ")\n",
        "\n",
        "# ìƒì„±ëœ ë³´í–‰ì ìˆ˜ í™•ì¸\n",
        "print(f\"Total pedestrians generated: {len(pedestrians)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b17ad5-d8dc-4a33-8bfb-f9cee50b3e39",
      "metadata": {
        "id": "30b17ad5-d8dc-4a33-8bfb-f9cee50b3e39",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dynamic_congestion_grid, accumulated_congestion_grid, evacuation_details, max_congestion_per_step= run_simulation(grid, pedestrians, evacuation_target_area_cells, x_coords, y_coords, area_gdf, buildings_gdf, 100, evacuation_target_area_gdf, output_dir, flood_cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a008f16-7e44-4e00-8e16-f9a1dca98269",
      "metadata": {
        "id": "2a008f16-7e44-4e00-8e16-f9a1dca98269"
      },
      "outputs": [],
      "source": [
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, accumulated_congestion_grid, x_coords, y_coords)\n",
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, dynamic_congestion_grid, x_coords, y_coords)\n",
        "save_results_to_excel(\n",
        "    pedestrians=pedestrians,\n",
        "    filename=r'C:\\Users\\doohu\\Desktop\\ëŒ€í•™ì›\\ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜_ìµœì¢…ì½”ë“œ\\ê²°ê³¼\\ê²°ê³¼1.xlsx',\n",
        "    accumulated_congestion_grid=accumulated_congestion_grid,\n",
        "    max_congestion_per_step=max_congestion_per_step\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ebcdea-20d5-4122-a226-981e403c322a",
      "metadata": {
        "id": "d8ebcdea-20d5-4122-a226-981e403c322a"
      },
      "outputs": [],
      "source": [
        "\n",
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, accumulated_congestion_grid, x_coords, y_coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32914476-3678-46e0-9a28-ec241d4e416d",
      "metadata": {
        "id": "32914476-3678-46e0-9a28-ec241d4e416d"
      },
      "outputs": [],
      "source": [
        "visualize_accumulated_congestion(area_gdf, buildings_gdf, evacuation_target_area_gdf, dynamic_congestion_grid, x_coords, y_coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b05377-673b-4545-9aa4-33b874274038",
      "metadata": {
        "id": "48b05377-673b-4545-9aa4-33b874274038"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf8d0e6-6212-49ed-b935-cde955cafbea",
      "metadata": {
        "id": "7bf8d0e6-6212-49ed-b935-cde955cafbea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a97052-bb09-4172-b4e0-4f3a046da83b",
      "metadata": {
        "id": "61a97052-bb09-4172-b4e0-4f3a046da83b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4bd9692-a1ea-4cfc-82b6-3d39e9f3048a",
      "metadata": {
        "id": "b4bd9692-a1ea-4cfc-82b6-3d39e9f3048a"
      },
      "outputs": [],
      "source": [
        "ã…ã„´ã…ã„´ã…ã„´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abd8ef0-04e4-43f7-bed7-5e3c37cf08b1",
      "metadata": {
        "id": "3abd8ef0-04e4-43f7-bed7-5e3c37cf08b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b1e27a-68e1-4ecf-9273-6fe343eed424",
      "metadata": {
        "id": "11b1e27a-68e1-4ecf-9273-6fe343eed424"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09460eb-761f-48bc-8328-cb4aa8341338",
      "metadata": {
        "id": "e09460eb-761f-48bc-8328-cb4aa8341338"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34cf90d1-b2a2-419f-b624-6b901ad21290",
      "metadata": {
        "id": "34cf90d1-b2a2-419f-b624-6b901ad21290"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e2f1a5-dab6-44e2-9259-d21e6f418f9f",
      "metadata": {
        "id": "30e2f1a5-dab6-44e2-9259-d21e6f418f9f"
      },
      "outputs": [],
      "source": [
        "ã…ã„´ã…ã„´ã…ã„´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16cf4da-9c1c-4f12-9c5d-4617b9740003",
      "metadata": {
        "id": "d16cf4da-9c1c-4f12-9c5d-4617b9740003"
      },
      "outputs": [],
      "source": [
        "from openpyxl import Workbook\n",
        "\n",
        "def save_evacuations_to_excel(pedestrians, evacuation_target_area_gdf, x_coords, y_coords, filename):\n",
        "    \"\"\"\n",
        "    ë³´í–‰ì ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥í•˜ë©°, ë‹¤ì–‘í•œ í†µê³„ ì‹œíŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
        "\n",
        "    Parameters:\n",
        "        pedestrians (list): ë³´í–‰ì ê°ì²´ ë¦¬ìŠ¤íŠ¸\n",
        "        evacuation_target_area_gdf (GeoDataFrame): ëŒ€í”¼ì†Œ ì˜ì—­ GeoDataFrame\n",
        "        x_coords, y_coords (numpy.ndarray): ê²©ì ì¢Œí‘œë¥¼ EPSG:5181ë¡œ ë³€í™˜í•˜ëŠ” ë°°ì—´\n",
        "        filename (str): ì €ì¥í•  Excel íŒŒì¼ ì´ë¦„\n",
        "    \"\"\"\n",
        "    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
        "    results = []\n",
        "    for pedestrian in pedestrians:\n",
        "        escape_time = round(pedestrian.total_distance / pedestrian.speed, 2) if pedestrian.speed > 0 else None\n",
        "\n",
        "        result = {\n",
        "            \"ID\": pedestrian.id,\n",
        "            \"í˜„ì¬ ìœ„ì¹˜\": pedestrian.position,\n",
        "            \"ì†ë„\": pedestrian.speed,\n",
        "            \"í–‰ë™ ìœ í˜•\": pedestrian.behavior,\n",
        "            \"ë³´í–‰ì íƒ€ì…\": pedestrian.type,\n",
        "            \"ì‹œì¥ ì§€ì—­ ì‹œì‘ ì—¬ë¶€\": pedestrian.special_area,\n",
        "            \"ëŒ€í”¼ì†Œ ì¸ì§€ ì—¬ë¶€\": pedestrian.knows_target,\n",
        "            \"ëŒ€í”¼ ëª©í‘œ\": pedestrian.goal,\n",
        "            \"ëŒ€í”¼ ì„±ê³µ ì—¬ë¶€\": pedestrian.goal_reached,\n",
        "            \"ì‹¤ì œ ê²½ë¡œ\": pedestrian.real_path,\n",
        "            \"ì´ ì´ë™ ê±°ë¦¬(ë¯¸í„°)\": pedestrian.total_distance,\n",
        "            \"ì†Œìš” ì‹œê°„(ì´ˆ)\": escape_time\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # DataFrame ìƒì„±\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Sheet 1: ê° ë³´í–‰ìë³„ ìƒì„¸ ê²°ê³¼\n",
        "    sheet1 = df\n",
        "\n",
        "    # Sheet 2: ëŒ€í”¼ ìœ í˜•ë³„ ëŒ€í”¼ í˜„í™©\n",
        "    behavior_stats = df.groupby(\"í–‰ë™ ìœ í˜•\").agg(\n",
        "        ì´_ë³´í–‰ììˆ˜=(\"ID\", \"count\"),\n",
        "        ëŒ€í”¼_ì„±ê³µììˆ˜=(\"ëŒ€í”¼ ì„±ê³µ ì—¬ë¶€\", \"sum\"),\n",
        "        ëŒ€í”¼_ì„±ê³µë¥ _í¼ì„¼íŠ¸=(\"ëŒ€í”¼ ì„±ê³µ ì—¬ë¶€\", lambda x: round(x.mean() * 100, 2)),\n",
        "        í‰ê· _ì†Œìš”ì‹œê°„=(\"ì†Œìš” ì‹œê°„(ì´ˆ)\", \"mean\"),\n",
        "        í‰ê· _ì´ë™ê±°ë¦¬=(\"ì´ ì´ë™ ê±°ë¦¬(ë¯¸í„°)\", \"mean\")\n",
        "    )\n",
        "    behavior_stats.index.name = \"í–‰ë™ ìœ í˜•\"\n",
        "\n",
        "    # Sheet 3: ë³´í–‰ì íƒ€ì…ë³„ ëŒ€í”¼ í˜„í™©\n",
        "    agent_type_stats = df.groupby(\"ë³´í–‰ì íƒ€ì…\").agg(\n",
        "        ì´_ë³´í–‰ììˆ˜=(\"ID\", \"count\"),\n",
        "        ëŒ€í”¼_ì„±ê³µììˆ˜=(\"ëŒ€í”¼ ì„±ê³µ ì—¬ë¶€\", \"sum\"),\n",
        "        ëŒ€í”¼_ì„±ê³µë¥ _í¼ì„¼íŠ¸=(\"ëŒ€í”¼ ì„±ê³µ ì—¬ë¶€\", lambda x: round(x.mean() * 100, 2)),\n",
        "        í‰ê· _ì†Œìš”ì‹œê°„=(\"ì†Œìš” ì‹œê°„(ì´ˆ)\", \"mean\"),\n",
        "        í‰ê· _ì´ë™ê±°ë¦¬=(\"ì´ ì´ë™ ê±°ë¦¬(ë¯¸í„°)\", \"mean\")\n",
        "    )\n",
        "    agent_type_stats.index.name = \"ë³´í–‰ì íƒ€ì…\"\n",
        "\n",
        "    # Sheet 4: ëŒ€í”¼ì†Œë³„ ëŒ€í”¼ í˜„í™©\n",
        "    evacuation_data = []\n",
        "    valid_goals = df[df[\"ëŒ€í”¼ ëª©í‘œ\"].notnull()]\n",
        "\n",
        "    geometry = []\n",
        "    for goal in valid_goals[\"ëŒ€í”¼ ëª©í‘œ\"]:\n",
        "        if isinstance(goal, tuple) and 0 <= goal[1] < len(x_coords) and 0 <= goal[0] < len(y_coords):\n",
        "            geometry.append(Point(x_coords[goal[1]], y_coords[goal[0]]))\n",
        "        else:\n",
        "            geometry.append(None)\n",
        "\n",
        "    pedestrian_goals = gpd.GeoDataFrame(\n",
        "        valid_goals,\n",
        "        geometry=geometry,\n",
        "        crs=\"EPSG:5181\"\n",
        "    ).dropna(subset=[\"geometry\"])\n",
        "\n",
        "    for idx, shelter in evacuation_target_area_gdf.iterrows():\n",
        "        shelter_name = f\"{shelter.get('A24', '')}{shelter.get('A25', '')}\"\n",
        "        shelter_geometry = shelter.geometry\n",
        "        reached_pedestrians = pedestrian_goals[pedestrian_goals.geometry.within(shelter_geometry)]\n",
        "\n",
        "        evacuation_data.append({\n",
        "            \"ëŒ€í”¼ì†Œ ì´ë¦„\": shelter_name,\n",
        "            \"ëŒ€í”¼ì†Œ ì¢Œí‘œ\": (shelter_geometry.centroid.x, shelter_geometry.centroid.y),\n",
        "            \"ë„ì°©í•œ ë³´í–‰ì ìˆ˜\": len(reached_pedestrians),\n",
        "            \"í‰ê·  ì†Œìš” ì‹œê°„(ì´ˆ)\": reached_pedestrians[\"ì†Œìš” ì‹œê°„(ì´ˆ)\"].mean() if len(reached_pedestrians) > 0 else None,\n",
        "            \"í‰ê·  ì´ë™ ê±°ë¦¬(ë¯¸í„°)\": reached_pedestrians[\"ì´ ì´ë™ ê±°ë¦¬(ë¯¸í„°)\"].mean() if len(reached_pedestrians) > 0 else None,\n",
        "            \"ë³´í–‰ì íƒ€ì…ë³„ ë„ì°© ìˆ˜\": reached_pedestrians[\"ë³´í–‰ì íƒ€ì…\"].value_counts().to_dict(),\n",
        "            \"í–‰ë™ ìœ í˜•ë³„ ë„ì°© ìˆ˜\": reached_pedestrians[\"í–‰ë™ ìœ í˜•\"].value_counts().to_dict()\n",
        "        })\n",
        "\n",
        "    shelter_stats = pd.DataFrame(evacuation_data)\n",
        "\n",
        "    # Excel íŒŒì¼ë¡œ ì €ì¥\n",
        "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
        "        sheet1.to_excel(writer, sheet_name=\"ë³´í–‰ì ìƒì„¸ ê²°ê³¼\", index=False)\n",
        "        behavior_stats.to_excel(writer, sheet_name=\"ëŒ€í”¼ ìœ í˜•ë³„ í˜„í™©\")\n",
        "        agent_type_stats.to_excel(writer, sheet_name=\"ë³´í–‰ì íƒ€ì…ë³„ í˜„í™©\")\n",
        "        shelter_stats.to_excel(writer, sheet_name=\"ëŒ€í”¼ì†Œë³„ í˜„í™©\", index=False)\n",
        "\n",
        "    print(f\"Evacuation results saved to '{filename}' with 4 sheets.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b819f88e-d2f2-4905-806f-c14fde26c289",
      "metadata": {
        "id": "b819f88e-d2f2-4905-806f-c14fde26c289"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "badcafd6-7773-4c23-94b5-a0b13f43874c",
      "metadata": {
        "id": "badcafd6-7773-4c23-94b5-a0b13f43874c"
      },
      "outputs": [],
      "source": [
        "save_evacuations_to_excel(pedestrians, r\"C:\\Users\\doohu\\Desktop\\digital_twin\\results\\evacuation_results4.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c788dc22-1359-4539-b60e-ea46acad35cc",
      "metadata": {
        "id": "c788dc22-1359-4539-b60e-ea46acad35cc"
      },
      "outputs": [],
      "source": [
        "filename = r\"C:\\Users\\doohu\\Desktop\\digital_twin\\results\\evacuation_results3.xlsx\"\n",
        "# Call the function\n",
        "save_evacuations_to_excel(\n",
        "    pedestrians=pedestrians,\n",
        "    evacuation_target_area_gdf=evacuation_target_area_gdf,\n",
        "    x_coords=x_coords,\n",
        "    y_coords=y_coords,\n",
        "    filename=filename\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e4241-5c77-492e-b0bd-bade331935f2",
      "metadata": {
        "id": "5b3e4241-5c77-492e-b0bd-bade331935f2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •\n",
        "base_dir = r\"C:\\Users\\doohu\\Desktop\\digital_twin\\20250106\"\n",
        "output_dir = os.path.join(base_dir, \"videos\")\n",
        "\n",
        "# í•˜ìœ„ ê²½ë¡œ ëª©ë¡\n",
        "sub_dirs = [\"by_behavior\", \"by_congestion\", \"by_type\"]\n",
        "\n",
        "# ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ step ë²ˆí˜¸ ì¶”ì¶œ\n",
        "step_pattern = re.compile(r\"step_(\\d+)\")\n",
        "\n",
        "# ë¹„ë””ì˜¤ ìƒì„± í•¨ìˆ˜\n",
        "def create_video_from_images(image_folder, output_video_path, frame_rate=24.0):\n",
        "    if not os.path.exists(image_folder):\n",
        "        print(f\"ì´ë¯¸ì§€ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {image_folder}\")\n",
        "        return\n",
        "\n",
        "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\") or img.endswith(\".jpg\")]\n",
        "\n",
        "    # ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ step ë²ˆí˜¸ ì¶”ì¶œ ë° ì •ë ¬\n",
        "    try:\n",
        "        images.sort(key=lambda x: int(step_pattern.search(x).group(1)) if step_pattern.search(x) else float('inf'))\n",
        "    except Exception as e:\n",
        "        print(f\"ì´ë¯¸ì§€ ì •ë ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return\n",
        "\n",
        "    if not images:\n",
        "        print(f\"{image_folder}ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # ì²« ì´ë¯¸ì§€ë¡œ í”„ë ˆì„ í¬ê¸° ì„¤ì •\n",
        "    first_image_path = os.path.join(image_folder, images[0])\n",
        "    print(f\"ì²« ì´ë¯¸ì§€ ê²½ë¡œ: {first_image_path}\")\n",
        "\n",
        "    if not os.path.exists(first_image_path):\n",
        "        print(f\"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {first_image_path}\")\n",
        "        return\n",
        "\n",
        "    frame = cv2.imread(first_image_path)\n",
        "\n",
        "    if frame is None:\n",
        "        print(f\"ì´ë¯¸ì§€ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {first_image_path}\")\n",
        "        return\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "    size = (width, height)\n",
        "\n",
        "    # VideoWriter ê°ì²´ ìƒì„±\n",
        "    os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, size)\n",
        "\n",
        "    # ì´ë¯¸ì§€ë“¤ì„ ë¹„ë””ì˜¤ì— ì¶”ê°€\n",
        "    for image in images:\n",
        "        img_path = os.path.join(image_folder, image)\n",
        "        frame = cv2.imread(img_path)\n",
        "        if frame is not None:\n",
        "            out.write(frame)\n",
        "        else:\n",
        "            print(f\"ì´ë¯¸ì§€ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {img_path}\")\n",
        "\n",
        "    out.release()\n",
        "    print(f\"ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_video_path}\")\n",
        "\n",
        "# ê° í•˜ìœ„ ê²½ë¡œì— ëŒ€í•´ ì˜ìƒ ìƒì„±\n",
        "for sub_dir in sub_dirs:\n",
        "    image_folder = os.path.join(base_dir, sub_dir)\n",
        "    output_video_path = os.path.join(output_dir, f\"{sub_dir}2.avi\")\n",
        "    print(f\"Processing: {sub_dir}\")\n",
        "    create_video_from_images(image_folder, output_video_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}